{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfridland/NLP/blob/HW5/NER_1_LMV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkARMEGI1lq_"
      },
      "source": [
        "## Тема разметка и извлечение именованных сущностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6BjKA8h1lrE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "21f0e3b3-3b7a-4214-da90-d901d44b8682"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-99ff09072fff>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (Named-entity recognition, NER)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "(Named-entity recognition, NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcnzVxam1lrG"
      },
      "source": [
        "### 3.1. Разметка слов с помощью частей речи (Parts-Of-Speech)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lOxR6QA1lrH"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZqfStcK1lrH"
      },
      "source": [
        "POS разметка - определение части речи и грамматических характеристик слов в тексте (корпусе) с приписыванием им соответствующих тегов. POS tagging является одним из первых этапов компьютерного анализа текста. Эта задача не простая, так как конкретное слово может иметь различную часть речи в зависимости от контекста, в котором оно используется.\n",
        "Например: в предложении “Дай мне свой ответ” ответ - это существительное, а в предложении “ответь на вопрос” ответ - это глагол.\n",
        "\n",
        "POS разметка полезна для построения деревьев синтаксического анализа, которые используются при построении NER (именованных сущностей) и извлечении отношений между словами. POS-маркировка также необходима для построения лемматизаторов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMu9X87l1lrI"
      },
      "outputs": [],
      "source": [
        "#nltk.download() \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYTgxEXM2YAM",
        "outputId": "84867a34-b289-4e81-be8b-69a7145ee30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnMezqer1lrI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfhTvMJw1lrI"
      },
      "source": [
        "#### Универсальный набор тегов части речи"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t6JTedc1lrJ"
      },
      "source": [
        "Ниже приведена таблица POS-тэгов: обозначение, часть речи, пример. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1neVO3LF1lrJ"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzDOEsc1lrJ"
      },
      "source": [
        "Выведим примеры тэгов разных частей речи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f13qX-t41lrK",
        "outputId": "ed4e7977-6b6c-4a37-f35e-aaa43b602e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n"
          ]
        }
      ],
      "source": [
        "nltk.help.upenn_tagset('RB')\n",
        "nltk.help.upenn_tagset('NN')\n",
        "nltk.help.upenn_tagset('VB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3hrD-qF1lrL"
      },
      "source": [
        "#### Сравнение POS тэггеров"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw96NpBK1lrL"
      },
      "source": [
        "POS-тэггер обрабатывает последовательность слов и определяет тэг части речи для каждого слова. Сравним работу нескольких тэггеров библиотеки nltk.tag. Проверять работоспособность теггеров будем на корпусе nltk.corpus.brown.\n",
        "\n",
        "Отобразим распределение тэгов в корпусе brown. Можем видеть, что тэг \"NN\" наиболее популярный.\n",
        "\n",
        "Корпус будет поделен на train и test, т.к. некоторым тэггерам необходимо обучение. Разметку визуально будем оценивать на примере test_sent, тестового предложения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKTmg4Qg1lrM",
        "outputId": "1dd649d0-4299-4faf-97ad-a4fce06d1931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J10XArVU1lrM",
        "outputId": "d59f3f5f-4dd7-4e1e-a452-2c1ad3eea1f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<LazyModule 'nltk.corpus'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nltk.corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il4TzQSu1lrN",
        "outputId": "a5bb1a70-9dd8-4f71-c71c-242d7765144a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'NN': 13162, 'IN': 10616, 'AT': 8893, 'NP': 6866, ',': 5133, 'NNS': 5066, '.': 4452, 'JJ': 4392, 'CC': 2664, 'VBD': 2524, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk.tag import DefaultTagger\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger, TrigramTagger\n",
        "from nltk.tag import RegexpTagger\n",
        "\n",
        "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
        "nltk.FreqDist(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaXGXDla1lrO",
        "outputId": "1d9a0b8d-1f46-4a73-9f94-1e67f73f1705"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('But', 'CC'),\n",
              " ('in', 'IN'),\n",
              " ('all', 'ABN'),\n",
              " ('its', 'PP$'),\n",
              " ('175', 'CD'),\n",
              " ('years', 'NNS'),\n",
              " (',', ','),\n",
              " ('not', '*'),\n",
              " ('a', 'AT'),\n",
              " ('single', 'AP'),\n",
              " ('Negro', 'NP'),\n",
              " ('student', 'NN'),\n",
              " ('has', 'HVZ'),\n",
              " ('entered', 'VBN'),\n",
              " ('its', 'PP$'),\n",
              " ('classrooms', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "train_data = brown_tagged_sents[:int(len(brown_tagged_sents) * 0.9)]\n",
        "test_data = brown_tagged_sents[int(len(brown_tagged_sents) * 0.9):]\n",
        "test_sent = brown.sents(categories='news')[0]\n",
        "test_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyV57I3D1lrP"
      },
      "source": [
        "#### DefaultTagger\n",
        "\n",
        "Очень наивный тэггер, в данном случае присваивает тэг \"NN\" (noun - существительное) всем словам в тексте. Т.к. в английском тексте примерно 13% существительных, то получим точность тэггирования примерно 0,13.\n",
        "\n",
        "У каждого теггера есть метод .tag(), который принимает список токенов (обычно список слов, созданных токенизатором слов), где каждый токен-это одно слово.\n",
        "Метод .evaluate() оценивает точность работы тэггера."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "QVna2my_1lrQ",
        "outputId": "b6808994-7b98-487a-f400-cf8e0884e797"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('The', 'NN'),\n",
              " ('Fulton', 'NN'),\n",
              " ('County', 'NN'),\n",
              " ('Grand', 'NN'),\n",
              " ('Jury', 'NN'),\n",
              " ('said', 'NN'),\n",
              " ('Friday', 'NN'),\n",
              " ('an', 'NN'),\n",
              " ('investigation', 'NN'),\n",
              " ('of', 'NN'),\n",
              " (\"Atlanta's\", 'NN'),\n",
              " ('recent', 'NN'),\n",
              " ('primary', 'NN'),\n",
              " ('election', 'NN'),\n",
              " ('produced', 'NN'),\n",
              " ('``', 'NN'),\n",
              " ('no', 'NN'),\n",
              " ('evidence', 'NN'),\n",
              " (\"''\", 'NN'),\n",
              " ('that', 'NN'),\n",
              " ('any', 'NN'),\n",
              " ('irregularities', 'NN'),\n",
              " ('took', 'NN'),\n",
              " ('place', 'NN'),\n",
              " ('.', 'NN')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.1262832652247583"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "display(default_tagger.tag(test_sent), default_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC4tvnWm1lrQ"
      },
      "source": [
        "#### UnigramTagger\n",
        "\n",
        "UnigramTagger учитывает условную частоту тегов и предсказывает наиболее частый тег для каждого токена, не ориентируется на соседние слова. Например: в предложении “Дай мне свой ответ” ответ - это существительное, а в предложении “ответь на вопрос” ответ - это глагол."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "5JYcyqkr1lrR",
        "outputId": "518e799e-4378-49aa-ef3b-e00ba7684dcd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " ('Fulton', 'NP-TL'),\n",
              " ('County', 'NN-TL'),\n",
              " ('Grand', 'JJ-TL'),\n",
              " ('Jury', 'NN-TL'),\n",
              " ('said', 'VBD'),\n",
              " ('Friday', 'NR'),\n",
              " ('an', 'AT'),\n",
              " ('investigation', 'NN'),\n",
              " ('of', 'IN'),\n",
              " (\"Atlanta's\", 'NP$'),\n",
              " ('recent', 'JJ'),\n",
              " ('primary', 'NN'),\n",
              " ('election', 'NN'),\n",
              " ('produced', 'VBD'),\n",
              " ('``', '``'),\n",
              " ('no', 'AT'),\n",
              " ('evidence', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('that', 'CS'),\n",
              " ('any', 'DTI'),\n",
              " ('irregularities', 'NNS'),\n",
              " ('took', 'VBD'),\n",
              " ('place', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8121200039868434"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "unigram_tagger = UnigramTagger(train_data)\n",
        "display(unigram_tagger.tag(test_sent), unigram_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCYZPs91lrR"
      },
      "source": [
        "#### BigramTagger\n",
        "\n",
        "BigramTagger учитывает тэги двух слов: текущее и предыдущее слово. Точность немного выше, чем у Unigram tagger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "QbZpcDH71lrS",
        "outputId": "4b5501ef-e4c9-4079-c8ae-f8adb4d34797"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " ('Fulton', 'NP-TL'),\n",
              " ('County', 'NN-TL'),\n",
              " ('Grand', 'JJ-TL'),\n",
              " ('Jury', 'NN-TL'),\n",
              " ('said', 'VBD'),\n",
              " ('Friday', 'NR'),\n",
              " ('an', 'AT'),\n",
              " ('investigation', 'NN'),\n",
              " ('of', 'IN'),\n",
              " (\"Atlanta's\", 'NP$'),\n",
              " ('recent', 'JJ'),\n",
              " ('primary', 'NN'),\n",
              " ('election', 'NN'),\n",
              " ('produced', 'VBD'),\n",
              " ('``', '``'),\n",
              " ('no', 'AT'),\n",
              " ('evidence', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('that', 'CS'),\n",
              " ('any', 'DTI'),\n",
              " ('irregularities', 'NNS'),\n",
              " ('took', 'VBD'),\n",
              " ('place', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8210904016744742"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
        "display(bigram_tagger.tag(test_sent), bigram_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7D5-WXH1lrS"
      },
      "source": [
        "#### TrigramTagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "bO9DVL9G1lrT",
        "outputId": "e5d34eb0-e457-482e-e6e2-2628222e960a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " ('Fulton', 'NP-TL'),\n",
              " ('County', 'NN-TL'),\n",
              " ('Grand', 'JJ-TL'),\n",
              " ('Jury', 'NN-TL'),\n",
              " ('said', 'VBD'),\n",
              " ('Friday', 'NR'),\n",
              " ('an', 'AT'),\n",
              " ('investigation', 'NN'),\n",
              " ('of', 'IN'),\n",
              " (\"Atlanta's\", 'NP$'),\n",
              " ('recent', 'JJ'),\n",
              " ('primary', 'NN'),\n",
              " ('election', 'NN'),\n",
              " ('produced', 'VBD'),\n",
              " ('``', '``'),\n",
              " ('no', 'AT'),\n",
              " ('evidence', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('that', 'CS'),\n",
              " ('any', 'DTI'),\n",
              " ('irregularities', 'NNS'),\n",
              " ('took', 'VBD'),\n",
              " ('place', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8185986245390212"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n",
        "display(trigram_tagger.tag(test_sent), trigram_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_glxG-gR1lrT"
      },
      "source": [
        "#### RegexpTagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsdnMNd91lrT"
      },
      "source": [
        "RegexpTagger работает на основе поиска совпаднения с регулярными выражениями. Пример: числа можно сопоставить с регулярным выражением \"\\d\" для присвоения тега CD (Cardinal number) или можно сопоставить известные шаблоны слов, такие как суффикс '.*ing$'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "KdMUhJwq1lrU",
        "outputId": "88616f11-2575-4701-8c6f-2394092c7295"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('The', 'NN'),\n",
              " ('Fulton', 'NN'),\n",
              " ('County', 'NN'),\n",
              " ('Grand', 'NN'),\n",
              " ('Jury', 'NN'),\n",
              " ('said', 'NN'),\n",
              " ('Friday', 'NN'),\n",
              " ('an', 'NN'),\n",
              " ('investigation', 'NN'),\n",
              " ('of', 'NN'),\n",
              " (\"Atlanta's\", 'NN$'),\n",
              " ('recent', 'NN'),\n",
              " ('primary', 'NN'),\n",
              " ('election', 'NN'),\n",
              " ('produced', 'VBD'),\n",
              " ('``', 'NN'),\n",
              " ('no', 'NN'),\n",
              " ('evidence', 'NN'),\n",
              " (\"''\", 'NN'),\n",
              " ('that', 'NN'),\n",
              " ('any', 'NN'),\n",
              " ('irregularities', 'VBZ'),\n",
              " ('took', 'NN'),\n",
              " ('place', 'NN'),\n",
              " ('.', 'NN')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.20253164556962025"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "patterns = [\n",
        "    (r'.*ing$', 'VBG'),                # gerunds\n",
        "    (r'.*ed$', 'VBD'),                 # simple past\n",
        "    (r'.*es$', 'VBZ'),                 # 3rd singular present\n",
        "    (r'.*ould$', 'MD'),                # modals\n",
        "    (r'.*\\'s$', 'NN$'),                # possessive nouns\n",
        "    (r'.*s$', 'NNS'),                  # plural nouns\n",
        "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
        "    (r'.*', 'NN'),                      # nouns (default) \n",
        "    (r'.*ment$', 'NN'),                # i.e. wonderment \n",
        "    (r'.*ful$', 'JJ')                  # i.e. wonderful \n",
        "]\n",
        "regexp_tagger = RegexpTagger(patterns)\n",
        "display(regexp_tagger.tag(test_sent), regexp_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vahKRFIi1lrU"
      },
      "source": [
        "Судя по результату, регулярных выражений в patterns не достаточно, чтобы покрыть большой процент слов корпуса."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9FfU9wy1lrU"
      },
      "source": [
        "#### Комбинация тэггеров\n",
        "\n",
        "Примущество Backoff Tagging в том, что если текущий тэггер не знает, как тэггировать слово, он передает это следующему и так далее, пока не пройдет перебор по всем тэггерам. В данному случае тэггирование производит последовательность UnigramTagger, BigramTagger, TrigramTagger. Комбинация тэггеров дала немного лучший результат, чем UnigramTagger, BigramTagger по отдельности."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90fq_ZfN1lrV",
        "outputId": "bef32b54-c53e-4916-c6d1-477696c222d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.843317053722715"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from nltk.tag import TrigramTagger \n",
        "\n",
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "backoff = DefaultTagger('NN') \n",
        "tag = backoff_tagger(train_data,  \n",
        "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
        "                     backoff = backoff) \n",
        "  \n",
        "tag.evaluate(test_data) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aLqH7hC1lrW"
      },
      "source": [
        "#### Создание тэггера имен \n",
        "\n",
        "Подходим к теме именнованых сущностей (NER). Мы можем самостоятельно написать необходимый тэггер. Допустим, мы хотим тэггировать имена людей в тексте. Для этого берем корпус имен (nltk.corpus.names), создаем класс тэггера, задаем логику: если слово входит в множество имен, присваиваем ему тэг имени \"NNP\", иначе тэг \"None\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('names')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U31jPEF8SMd",
        "outputId": "c06b596d-0a70-471c-f9d1-c8cb8fa3697c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3yQiPn1lrW"
      },
      "source": [
        "![image-2.png](attachment:image-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2y27Yya1lrX",
        "outputId": "fca59326-408f-40e9-aff6-ad1b91dbfe1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Katya', 'NNP')]\n",
            "[('Adam', 'NNP')]\n",
            "[('Window', None)]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tag import SequentialBackoffTagger\n",
        "from nltk.corpus import names\n",
        "\n",
        "class NamesTagger(SequentialBackoffTagger):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
        "        self.name_set = set([n.lower() for n in names.words()])\n",
        "            \n",
        "    def choose_tag(self, tokens, index, history):\n",
        "        word = tokens[index]\n",
        "        if word.lower() in self.name_set:\n",
        "             return 'NNP'\n",
        "        else:\n",
        "             return None\n",
        "            \n",
        "nt = NamesTagger()\n",
        "print(nt.tag(['Katya'])) \n",
        "print(nt.tag(['Adam'])) \n",
        "print(nt.tag(['Window']))            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-iQXIHt1lrY"
      },
      "source": [
        "Разметка предложений\n",
        "\n",
        "На вход тэггеру также можно подавать предложения, self.tag() будет применяться к каждому слову предложения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjrQJXMO1lrZ",
        "outputId": "f566a1fe-44c2-4b0e-e800-fc8504175901"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('make', 'VB'), ('America', 'NP-TL'), ('great', 'JJ'), ('again', 'RB')],\n",
              " [('winter', 'NN'), ('is', 'BEZ'), ('coming', 'VBG')]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "bigram_tagger.tag_sents([['make', 'America', 'great', 'again'], ['winter', 'is', 'coming']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzMaCks1lra"
      },
      "source": [
        "####  Разметка корпусов\n",
        "\n",
        "Некоторые из корпусов, включенных в NLTK, были уже размечены. Вот пример того, что вы можете увидеть, если откроете nltk.corpus.brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nczYAJB1lra",
        "outputId": "336b99bd-2b2a-497d-d698-0cdc67d52634"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "nltk.corpus.brown.tagged_words()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJHXl_z4-hki",
        "outputId": "076aed11-f414-4ef9-a9cb-0588a7b82359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j3Zw79X1lrb"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_mcJp2E1lrc",
        "outputId": "b174b605-4ecb-4dbb-e0c2-7706d403365f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "nltk.corpus.brown.tagged_words(tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNwbzr1o1lrc"
      },
      "source": [
        "Частота частей речи в корпусе"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKTZXMXc1lre",
        "outputId": "acb6bb1b-3eb6-4b84-d0d8-0775c2375a11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NOUN', 13354),\n",
              " ('VERB', 12274),\n",
              " ('.', 10929),\n",
              " ('DET', 8155),\n",
              " ('ADP', 7069),\n",
              " ('PRON', 5205),\n",
              " ('ADV', 3879),\n",
              " ('ADJ', 3364),\n",
              " ('PRT', 2436),\n",
              " ('CONJ', 2173),\n",
              " ('NUM', 466),\n",
              " ('X', 38)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "brown_news_tagged = nltk.corpus.brown.tagged_words(categories='adventure', tagset='universal')\n",
        "tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
        "tag_fd.most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_x2FwNm1lrg",
        "outputId": "469c56b3-8211-424c-cccf-5ad86f8c22a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyconll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hve7ApAk1lrg"
      },
      "outputs": [],
      "source": [
        "import pyconll"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ru_syntagrus-ud-test.conllu https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-test.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuj8RbqvsBZN",
        "outputId": "5d02733f-2c7e-4bf5-99a5-1a7148b69dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-18 14:18:07--  https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-test.conllu\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu [following]\n",
            "--2023-05-18 14:18:08--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14970950 (14M) [text/plain]\n",
            "Saving to: ‘ru_syntagrus-ud-test.conllu’\n",
            "\n",
            "ru_syntagrus-ud-tes 100%[===================>]  14.28M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-05-18 14:18:09 (194 MB/s) - ‘ru_syntagrus-ud-test.conllu’ saved [14970950/14970950]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ru_syntagrus-ud-train.conllu https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-train-a.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLqFF8SosNcc",
        "outputId": "b53dcb37-3a0b-4cd2-de00-5c16ab977fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-18 14:11:08--  https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu [following]\n",
            "--2023-05-18 14:11:08--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40736581 (39M) [text/plain]\n",
            "Saving to: ‘ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "ru_syntagrus-ud-tra 100%[===================>]  38.85M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-18 14:11:10 (296 MB/s) - ‘ru_syntagrus-ud-train.conllu’ saved [40736581/40736581]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('ru_syntagrus-ud-test.conllu')"
      ],
      "metadata": {
        "id": "L7wPQgq1suZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9PjT4KdEvDtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYExhMBI1lri"
      },
      "outputs": [],
      "source": [
        "for sent in full_train[:2]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFTEofU71lri"
      },
      "outputs": [],
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsUYMNBV1lrj",
        "outputId": "906ff3ff-ead2-4b82-e996-7d5b45ee2c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Наибольшая длина предложения 194\n",
            "Наибольшая длина токена 31\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#sent = [sent.rstrip() for s in sent if not s.isspace()]\n",
        "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
        "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
        "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
        "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHcuqiTL1lrj",
        "outputId": "bdc538f7-ad60-43b2-acba-a9e091a2a62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-b2f0942eb246>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_train_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_test_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_test_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-b2f0942eb246>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_train_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_test_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_test_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 10: expected str instance, NoneType found"
          ]
        }
      ],
      "source": [
        "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
        "all_test_texts = [' '.join(token.form for token in sent) for sent in full_test]\n",
        "\n",
        "all_train_labels = [' '.join(token.form for token in sent) for sent in full_train]\n",
        "all_test_labels = [' '.join(token.form for token in sent) for sent in full_test]\n",
        "print('\\n'.join(all_train_texts[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3ZPBYTM1lrk"
      },
      "outputs": [],
      "source": [
        "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cTv-U1t1lrk"
      },
      "outputs": [],
      "source": [
        "display(bigram_tagger.tag(fdata_sent_test[100]), bigram_tagger.evaluate(fdata_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd_TEUi71lrl"
      },
      "outputs": [],
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w9O3qHP1lrl"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "#import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JdpcP0Q1lrl"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCukH5HK1lrm"
      },
      "outputs": [],
      "source": [
        "test_enc_labels = le.transform(test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh5FOs9n1lrm"
      },
      "outputs": [],
      "source": [
        "le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvgCDwdh1lrn"
      },
      "outputs": [],
      "source": [
        "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htb3lYPI1lrn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnwOT_711lrn"
      },
      "outputs": [],
      "source": [
        "X_train = hvectorizer.fit_transform(train_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmjvEQQZ1lro"
      },
      "outputs": [],
      "source": [
        "X_test = hvectorizer.transform(test_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flz2I_Vc1lro"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqxQ5O7b1lrp"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(random_state=0, max_iter=10)\n",
        "lr.fit(X_train, train_enc_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZkJDILw1lrq"
      },
      "outputs": [],
      "source": [
        "pred = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KmFE8bA1lrq"
      },
      "outputs": [],
      "source": [
        "accuracy_score(test_enc_labels, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSXvg6311lrr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDB3pj651lrr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkG_2MWH1lrr"
      },
      "source": [
        "### 3.4.  Извлечение именованных сущностей (NER) и отношений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkzOs7QG1lrr"
      },
      "source": [
        "Одна из самых популярных задач NLP – извлечении именованных сущностей (Named-entity recognition, NER). Задача NER – выделить спаны сущностей в тексте (спан – непрерывный фрагмент текста). Допустим, есть новостной текст, и мы хотим выделить в нем сущности (некоторый заранее зафиксированный набор — например, персоны, локации, организации, даты и так далее). Задача NER – понять, что участок текста “1 января 1997 года” является датой, “Кофи Аннан” – персоной, а “ООН” – организацией.\n",
        "\n",
        "##### Зачем?\n",
        "1.  Cтруктуризация неструктурированных данных. Пусть у вас есть какой-то текст (или набор текстов), и данные из него нужно ввести в базу данных (таблицу). \n",
        "2. Шаг в сторону “понимания” текста (контекста). Это может как иметь самостоятельную ценность, так и помочь лучше решать другие задачи NLP. Без решения задачи NER тяжело представить себе решение многих задач NLP, допустим, разрешение местоименной анафоры или построение вопросно-ответных систем. \n",
        "\n",
        "Пример 1: Местоименная анафора позволяет нам понять, к какому элементу текста относится местоимение. Например, пусть мы хотим проанализировать текст “Прискакал Чарминг на белом коне. Принцесса выбежала ему навстречу и поцеловала его”. Если мы выделили на слове “Чарминг” сущность Персона, то машина сможет намного легче понять, что принцесса, скорее всего, поцеловала не коня, а принца Чарминга.\n",
        "\n",
        "Пример 2: Теперь приведем пример, как выделение именованных сущностей может помочь при построении вопросно-ответных систем. Если задать в вашем любимом поисковике вопрос «Кто играл роль Дарта Вейдера в фильме “Империя наносит ответный удар”», то с большой вероятностью вы получите верный ответ. Это делается как раз с помощью выделения именованных сущностей: выделяем сущности (фильм, роль и т. п.), понимаем, что нас спрашивают, и дальше ищем ответ в базе данных.\n",
        "\n",
        "Для решения NER-задач существуют множество инструментов. Рассмотрим несколько из них.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8adzbTjG1lrs"
      },
      "source": [
        "#### NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFkAUmVs1lrs"
      },
      "source": [
        "Для разметки NER с помощью NLTK сначала производим токенизацию слов, затем POS тэггинг. \n",
        "В качестве документа возьмем статью 'https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news, распарсим ее с помощью BeautifulSoup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-O265G31lrs"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "ebkRQv8SPk54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD22fmxn1lrt"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def url_to_string(url):\n",
        "    res = requests.get(url)\n",
        "    html = res.text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    for script in soup([\"script\", \"style\", 'aside']):\n",
        "        script.extract()\n",
        "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
        "\n",
        "document = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n",
        "\n",
        "nltk.pos_tag(nltk.word_tokenize(document))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcP1xY9-1lrt"
      },
      "source": [
        "С помощью функции nltk.ne_chunk () (уже оттренированный классификатор) мы можем распознавать именованные сущности с помощью классификатора, который добавляет метки категорий, такие как PERSON, ORGANIZATION и GPE."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "id": "Di17ko_lPzI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "FvkI1HFPP9RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tEQThN51lrt"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-q0_FBWD1lru"
      },
      "outputs": [],
      "source": [
        "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDYhV4Ap1lru"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKGTg7yV1lrv"
      },
      "source": [
        "#### Spacy\n",
        "\n",
        "Spacy значительно быстрее NLTK, так как она написана на Cython и работает с объектами.\n",
        "Для NER Spacy работает как простой классификатор (неглубокая нейронная сеть с одним скрытым слоем). Объект Doc хранит последовательности токенов и все их аннотации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_wj6Zi1lrv"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG4IVW5p1lrv"
      },
      "outputs": [],
      "source": [
        "#!pip install -U spacy\n",
        "#python -m spacy info\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm\n",
        "#nlp = spacy.load('en_core_web_md')\n",
        "#nlp = spacy.load('en_core_web_sm')\n",
        "nlp = en_core_web_sm.load()\n",
        "ny_bb = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n",
        "article = nlp(ny_bb)\n",
        "displacy.render(article, jupyter=True, style='ent')"
      ],
      "metadata": {
        "id": "QR9nsRybRu6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFOZk4al1lrw"
      },
      "source": [
        "#### Deeppavlov\n",
        "\n",
        "DeepPavlov-это библиотека ИИ с открытым исходным кодом, построенная на TensorFlow и Keras. DeepPavlov предназначена для разработки чат-ботов и сложных разговорных систем, исследований в области nlp и, в частности, диалоговых систем.\n",
        "\n",
        "DeepPavlov использует несколько более новый вариант глубокой нейронной архитектуры Flair, известный как гибридная модель Bi-LSTM-CRF.\n",
        "\n",
        "\n",
        "Используем предтренированную модель \"ner_ontonotes_bert_mult\", которая работает с разными языками, в том числе и русским. Подадим на распознование предложение на русском языке. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZgjbkzVS1lrw"
      },
      "outputs": [],
      "source": [
        "# !python -m venv env \n",
        "# #.\\env\\Scripts\\activate.bat\n",
        "# !pip install deeppavlov\n",
        "# !python -m deeppavlov install squad_bert\n",
        "\n",
        "#!python -m deeppavlov install ner_ontonotes\n",
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model\n",
        "deeppavlov_ner = build_model(configs.ner.ner_rus, download=True)\n",
        "rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
        "deeppavlov_ner([rus_document])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwbAEiIU1lrw"
      },
      "source": [
        "### 3.4. Извлечение отношений\n",
        "\n",
        "Можно извлекать не только именнованные сущности (NER), но и отношения между словами в предложении (Relation extraction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR0MZivl1lrx"
      },
      "source": [
        "#### Spacy\n",
        "\n",
        "В уже знакомой нам библеотеке Spacy используем встроенные displaCy визуализатор со style=\"dep\" для отображения отношений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yNyTT3y-1lrx"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "#install spacy model\n",
        "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.0/en_core_web_md-2.2.0.tar.gz\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "doc = nlp(\"I think Barack Obama met founder of Facebook at occasion of a release of a new NLP algorithm.\")\n",
        "\n",
        "displacy.render(doc, style=\"dep\") # (1)\n",
        "displacy.render(doc, style=\"ent\") # (2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoCP36iV1lry"
      },
      "source": [
        "#### NLTK. Извлечение отношений\n",
        "\n",
        "Распарсим документ nltk.corpus.ieer.parsed_docs('NYT_19980315'), extract_rels позволяет нам выделять необходимые отношения, в данном случае мы ищем пары сущностей <'ORG', 'LOC'>. Также мы указали, что текст должен подходить под регулярное выражение IN.\n",
        "IEER корпус отмаркирован для именнованных сущностей."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('ieer')"
      ],
      "metadata": {
        "id": "c1Q3TQ1IopoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs_WH_Gi1lry"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
        "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
        "     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
        "                                      corpus='ieer', pattern = IN):\n",
        "            print(nltk.sem.rtuple(rel))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxp89Pzc1lrz"
      },
      "source": [
        "#### Allennlp\n",
        "\n",
        "Попробовать извлекать отношения онлайн можно с помощью демо Allennlp  https://demo.allennlp.org/dependency-parsing/. \n",
        "Allen NLP предлагает две модели NER с различными архитектурами: Gated Recurrent Unit (GRU) Network, bi-LSTM-CRF model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxZGk2Cf1lrz"
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUaO4jTD1lrz"
      },
      "outputs": [],
      "source": [
        "from pymorphy2 import MorphAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwPjdr591lr0"
      },
      "outputs": [],
      "source": [
        "morpher = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR1hjQpI1lr0"
      },
      "outputs": [],
      "source": [
        "morpher.parse(\"кошка\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfkfDga1lr0"
      },
      "source": [
        "Интерфейс Natasha более многословный. Пользователь явно инициализирует компоненты: загружает предобученные эмбеддинги, передаёт их в конструкторы моделей. Сам вызывает методы segment, tag_morph, parse_syntax для сегментации на токены и предложения, анализа морфологии и синтаксиса.\n",
        "Модуль извлечения именованных сущностей не зависит от результатов морфологического и синтаксического разбора, его можно использовать отдельно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiKTvNw31lr0"
      },
      "outputs": [],
      "source": [
        "!pip install natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHpBo7xK1lr1"
      },
      "outputs": [],
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OBHNk_K1lr1"
      },
      "outputs": [],
      "source": [
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)\n",
        "\n",
        "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
        "doc = Doc(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyPpSel-1lr2"
      },
      "outputs": [],
      "source": [
        "doc.segment(segmenter)\n",
        "display(doc.tokens[:5])\n",
        "display(doc.sents[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4eLDWTC1lr2"
      },
      "outputs": [],
      "source": [
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "doc.parse_syntax(syntax_parser)\n",
        "sent = doc.sents[0]\n",
        "sent.morph.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyUlVCov1lr3"
      },
      "outputs": [],
      "source": [
        "sent.syntax.print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxctaQQ51lr3"
      },
      "source": [
        "Модуль извлечения именованных сущностей не зависит от результатов морфологического и синтаксического разбора, его можно использовать отдельно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os6MD1fL1lr4"
      },
      "outputs": [],
      "source": [
        "doc.tag_ner(ner_tagger)\n",
        "doc.ner.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo0FIloE1lr4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEPtCu0a1lr4"
      },
      "source": [
        "https://natasha.github.io/corus/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGWMW6C01lr5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}