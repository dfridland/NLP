{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660a6954-680a-4200-b9ad-638239170ea7",
   "metadata": {},
   "source": [
    "\n",
    "Задание 1.\n",
    "Задание: обучите три классификатора:\n",
    "\n",
    "1) на токенах с высокой частотой\n",
    "\n",
    "2) на токенах со средней частотой\n",
    "\n",
    "3) на токенах с низкой частотой\n",
    "\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации.\n",
    "\n",
    "Задание 2.\n",
    "найти фичи с наибольшей значимостью, и вывести их\n",
    "\n",
    "Задание 3.\n",
    "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
    "\n",
    "2) подобрать оптимальный размер для hashing векторайзера\n",
    "\n",
    "3) убедиться что для сетки нет переобучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1164aeca-4f70-427b-a7e0-89ca34efcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk import tokenize as tknz\n",
    "from nltk import ngrams\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9deb4521-5b51-4261-be17-9b258c741d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evamelissatasdemir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcfe98d4-d780-4198-9f10-83996633b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/evamelissatasdemir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/evamelissatasdemir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/evamelissatasdemir/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565f9237-02a2-4976-9d16-48e643907c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75b7e82-5dc8-4559-945f-e6aef7546fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efacf214-e78e-4a53-85d8-97036b5027a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ede2f11-a9f7-4cfe-857a-567d8847ab0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ну любишь или нет? — Я не знаю кто ты бля:D ht...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @SpoonLamer: Ох,900 :D ну это конечно же @t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @veregijytaqo: У тебя есть ухажёр? Нет - мо...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Поприветствуем моего нового читателя @Alexey17...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Теперь у меня есть частичка Сиднея :) #Sydney ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  @first_timee хоть я и школота, но поверь, у на...  positive\n",
       "1  Да, все-таки он немного похож на него. Но мой ...  positive\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...  positive\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...  positive\n",
       "5  ну любишь или нет? — Я не знаю кто ты бля:D ht...  positive\n",
       "6  RT @SpoonLamer: Ох,900 :D ну это конечно же @t...  positive\n",
       "7  RT @veregijytaqo: У тебя есть ухажёр? Нет - мо...  positive\n",
       "8  Поприветствуем моего нового читателя @Alexey17...  positive\n",
       "9  Теперь у меня есть частичка Сиднея :) #Sydney ...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a887ee8-0dab-4cad-9868-058e324220ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 226834 entries, 0 to 111922\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    226834 non-null  object\n",
      " 1   label   226834 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b43f3721-b904-4a71-a890-775c5b6a4f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      хоть я и школота, но поверь, у нас то же сам...\n",
       "0    на работе был полный пиддес :| и так каждое за...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_word_user(input_txt):\n",
    "    pattern = \"@[\\w]*\"\n",
    "    lst = re.findall(pattern, input_txt)\n",
    "    result = input_txt\n",
    "    for el in lst:\n",
    "        result = re.sub(el, ' ', result)\n",
    "    return result\n",
    "\n",
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67a6f9d-7d9b-48fd-b8ae-216deda39c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>хоть я и школота, но поверь, у нас то же сам...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT  : Ну ты идиотка) я испугалась за тебя!!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT  : \"Кто то в углу сидит и погибает от голод...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Вот что значит страшилка :D\\nНо блин,посмотр...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0    хоть я и школота, но поверь, у нас то же сам...  positive\n",
       "1  Да, все-таки он немного похож на него. Но мой ...  positive\n",
       "2       RT  : Ну ты идиотка) я испугалась за тебя!!!  positive\n",
       "3  RT  : \"Кто то в углу сидит и погибает от голод...  positive\n",
       "4    Вот что значит страшилка :D\\nНо блин,посмотр...  positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = np.vectorize(remove_word_user)(df['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772b90d1-189e-4394-b2ce-fb10953f02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f1f536-bed5-4175-99fa-c3c37cc09c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(token, noise):\n",
    "    \"\"\" \n",
    "    Removing noise from tokens.\n",
    "    \"\"\"\n",
    "    remove_sw = [word for word in token if not word in noise]\n",
    "    return  remove_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f45a028-0a7d-4d13-94cf-22e0eddf1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatizer(words, lemmatizer, pos):\n",
    "    \"\"\"\n",
    "    lemmitization\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemmas.append(lemmatizer.lemmatize(word, pos = nltk.corpus.wordnet.VERB) )\n",
    "         \n",
    "    return lemmas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f6463a3-be1e-4c19-8cf9-f5780e2f795c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 8)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Input \u001b[1;32mIn [39]\u001b[0m in \u001b[1;35m<cell line: 1>\u001b[0m\n    get_ipython().run_cell_magic('time', '', \"\\n# Tokenizing\\n# Removing stop-words and punctuation\\n# Lemmatization\\n\\ndf['text_tokens'] = df['text'].apply(tknz.word_tokenize)\\ndf['text_tokens'] = df['text_tokens'].apply(remove_noise, noise=noise)\\ndf['text_tokens'] = \\\\\\n\\n        df['text_tokens'].apply(get_lemmatizer, lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB)\\ndf.head(3)\\n\")\n",
      "  File \u001b[1;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2358\u001b[0m in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[1;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magics/execution.py:1272\u001b[0m in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/compilerop.py:105\u001b[0;36m in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:8\u001b[0;36m\u001b[0m\n\u001b[0;31m    df['text_tokens'] = \\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenizing\n",
    "# Removing stop-words and punctuation\n",
    "# Lemmatization\n",
    "\n",
    "df['text_tokens'] = df['text'].apply(tknz.word_tokenize)\n",
    "df['text_tokens'] = df['text_tokens'].apply(remove_noise, noise=noise)\n",
    "df['text_tokens'] = \\\n",
    "\n",
    "        df['text_tokens'].apply(get_lemmatizer, lemmatizer = WordNetLemmatizer(), pos = wordnet.VERB)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25908f22-6e68-49ac-8d9d-84e9e760b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_corp = CountVectorizer(analyzer='word', \n",
    "                                  token_pattern=r'\\w{1,}')\n",
    "\n",
    "tfidf_vec_corp = TfidfVectorizer(analyzer='word', \n",
    "                                 token_pattern=r'\\w{1,}')\n",
    "\n",
    "count_vect_corp_mod = CountVectorizer(max_df=0.9, \n",
    "                                      max_features = 1000, \n",
    "                                      stop_words='russian', \n",
    "                                      analyzer='word', \n",
    "                                      token_pattern=r'\\w{1,}')\n",
    "\n",
    "tfidf_vec_corp_mod = TfidfVectorizer(max_df=0.9, \n",
    "                                     max_features = 1000, \n",
    "                                     stop_words='russian', \n",
    "                                     analyzer='word', \n",
    "                                     token_pattern=r'\\w{1,}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
