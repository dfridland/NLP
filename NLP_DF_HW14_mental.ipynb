{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfridland/NLP/blob/HW14/NLP_DF_HW14_mental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8642cb",
      "metadata": {
        "id": "7f8642cb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "056866c2",
      "metadata": {
        "id": "056866c2"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with open('./Mental/combined_dataset.json', 'r') as json_file:\n",
        "    json_list = list(json_file)\n",
        "for json_str in json_list:\n",
        "    data.append(json.loads(json_str)[\"Response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14ccee9",
      "metadata": {
        "id": "b14ccee9",
        "outputId": "b6b9bbfc-9d11-465a-eac2-6905e2ce5951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\",\n",
              " 'Hello, and thank you for your question and seeking advice on this. Feelings of worthlessness is unfortunately common. In fact, most people, if not all, have felt this to some degree at some point in their life. You are not alone.\\xa0Changing our feelings is like changing our thoughts - it\\'s hard to do. Our minds are so amazing that the minute you change your thought another one can be right there to take it\\'s place. Without your permission, another thought can just pop in there. The new thought may feel worse than the last one! My guess is that you have tried several things to improve this on your own even before reaching out on here. People often try thinking positive thoughts, debating with their thoughts, or simply telling themselves that they need to \"snap out of it\" - which is also a thought that carries some self-criticism.\\xa0Some people try a different approach, and there are counselors out there that can help you with this. The idea is that instead of trying to change the thoughts, you change how you respond to them. You learn skills that allow you to manage difficult thoughts and feelings differently so they don\\'t have the same impact on you that they do right now. For some people, they actually DO begin to experience less hurtful thoughts once they learn how to manage the ones they have differently. Acceptance and Commitment Therapy may be a good choice for you.\\xa0There is information online and even self-help books that you can use to teach you the skills that I mentioned. Because they are skills, they require practice, but many people have found great relief and an enriched life by learning them.\\xa0As for suicidal thoughts, I am very glad to read that this has not happened to you. Still, you should watch out for this because it can be a sign of a worsening depression. If you begin to think about this, it is important to reach out to a support system right away. The National Suicide Prevention Lifeline is 1-800-273-8255. The text line is #741741.\\xa0I hope some other colleagues will provide you more suggestions.\\xa0Be well...Robin Landwehr, DBH, LPCC']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e59b406",
      "metadata": {
        "id": "4e59b406",
        "outputId": "5e18aacb-0e16-467d-ade9-c584c7f776a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3512"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072cd1b0",
      "metadata": {
        "id": "072cd1b0"
      },
      "outputs": [],
      "source": [
        "#data = data[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3aeec92",
      "metadata": {
        "id": "e3aeec92"
      },
      "outputs": [],
      "source": [
        "def build_text_files(data_json, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    for texts in data_json:\n",
        "        summary = str(texts).strip()\n",
        "        # summary = re.sub(r'<.*?>', \" \", summary) # Убираем никнеймы\n",
        "        summary = re.sub(r\"\\s\", \" \", summary)\n",
        "        data += summary + \"  \"\n",
        "    f.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ad1cc8",
      "metadata": {
        "id": "74ad1cc8"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(data, test_size=0.15)\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a22d10",
      "metadata": {
        "id": "49a22d10",
        "outputId": "fdfd5707-4f1d-40c1-927c-e4b897acc217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset length: 2985\n",
            "Test dataset length: 527\n"
          ]
        }
      ],
      "source": [
        "print(\"Train dataset length: \"+ str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae409402",
      "metadata": {
        "id": "ae409402"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6727c5f",
      "metadata": {
        "id": "a6727c5f",
        "outputId": "2cbaa93d-c6b4-46b8-ab57-0331e5ab1445"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evamelissatasdemir/miniforge3/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5d004e",
      "metadata": {
        "id": "8f5d004e"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0af56c5",
      "metadata": {
        "id": "f0af56c5"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt_bash\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ea234c",
      "metadata": {
        "id": "44ea234c"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8e60a1",
      "metadata": {
        "id": "cf8e60a1",
        "outputId": "9f9c1633-9cef-42f3-fef2-ed2c09673295"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evamelissatasdemir/miniforge3/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1641' max='1641' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1641/1641 10:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.583200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.678400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>6.459300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1641, training_loss=6.8602676159139815, metrics={'train_runtime': 647.9332, 'train_samples_per_second': 10.131, 'train_steps_per_second': 2.533, 'total_flos': 428780224512000.0, 'train_loss': 6.8602676159139815, 'epoch': 3.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5c3f25",
      "metadata": {
        "id": "ec5c3f25"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389f3130",
      "metadata": {
        "id": "389f3130"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained('gpt_mental_bash')\n",
        "model.save_pretrained('model_gpt_mental_bash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee15488",
      "metadata": {
        "id": "4ee15488"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt_mental_bash\")\n",
        "model1 = AutoModelForCausalLM.from_pretrained(\"model_gpt_mental_bash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9238fc47",
      "metadata": {
        "id": "9238fc47"
      },
      "outputs": [],
      "source": [
        "prefix = \"I am not motivated \" # \"Инкапсуляция, наследование, полиморфизм\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a80173d",
      "metadata": {
        "id": "5a80173d"
      },
      "outputs": [],
      "source": [
        "def generate(prefix, gen_legth=50):\n",
        "\n",
        "    tokens = tokenizer(prefix, return_tensors='pt')\n",
        "\n",
        "    size = tokens['input_ids'].shape[1]\n",
        "    output = model1.generate(\n",
        "        **tokens,\n",
        "        #end_token=end_token_id,\n",
        "        do_sample=False,\n",
        "        max_length=size+gen_legth,\n",
        "        repetition_penalty=5.,\n",
        "        temperature=0.5,\n",
        "        num_beams=10,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0])\n",
        "    result = decoded[len(prefix):]\n",
        "    print(prefix + result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb13ac83",
      "metadata": {
        "id": "eb13ac83",
        "outputId": "636350c9-4de2-436a-a5db-85ac93e83d5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am not motivated ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ�\n"
          ]
        }
      ],
      "source": [
        "generate(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa43f2e0",
      "metadata": {
        "id": "fa43f2e0",
        "outputId": "55cc4c92-c821-4c95-9007-dececcfe6bde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What should I do not to cry?\n",
            "\n",
            "I don't know what you're talking about, but there's a lot of things that can go wrong in your life. You need to be able to take care of yourself and make it through the day without having to worry about anything else. It doesn't matter how much money you have or how long you've been away from home. There are so many things that could go wrong if you aren't taking care of yourself right now. The best way to deal with these problems is to\n"
          ]
        }
      ],
      "source": [
        "generate('What should I do not to cry?', gen_legth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d647d6",
      "metadata": {
        "id": "93d647d6"
      },
      "outputs": [],
      "source": [
        "def find_sentence(filename, sentence):\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            stringA = line\n",
        "    match = re.search(sentence, stringA)\n",
        "\n",
        "    if match:\n",
        "        print('Yes!')\n",
        "    else:\n",
        "        print('No!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a3ae93",
      "metadata": {
        "id": "87a3ae93"
      },
      "outputs": [],
      "source": [
        "find_sentence('./train_dataset.txt', 'Вставай и иди на кухню пить чай с печеньем.')\n",
        "find_sentence('./test_dataset.txt', 'Вставай и иди на кухню пить чай с печеньем')\n",
        "find_sentence('./train_dataset.txt', '<REal_SM[techsupport]>')\n",
        "find_sentence('./train_dataset.txt', \"Протестовать будем методом отключения мобильных телефонов на максимально длительный период.\")\n",
        "# Предыдущая фраза есть в трейновом датасете.\n",
        "find_sentence('./test_dataset.txt', \"Протестовать будем методом отключения мобильных телефонов на максимально длительный период.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}