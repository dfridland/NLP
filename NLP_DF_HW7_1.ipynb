{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfridland/NLP/blob/HW7/NLP_DF_HW7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eafad80b",
      "metadata": {
        "id": "eafad80b",
        "outputId": "44194eb3-b09c-4265-ca85-c11b283d688a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/evamelissatasdemir/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.callbacks import TensorBoard \n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import EarlyStopping  \n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cfca82",
      "metadata": {
        "id": "d5cfca82"
      },
      "outputs": [],
      "source": [
        "#f-score\n",
        "import keras.backend as K\n",
        "\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    \n",
        "    return f1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7904f32",
      "metadata": {
        "id": "b7904f32",
        "outputId": "cb36f3fe-459c-4c9a-c240-fb77f368745b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlrd in /Users/evamelissatasdemir/miniforge3/lib/python3.10/site-packages (2.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b7596f",
      "metadata": {
        "id": "a1b7596f",
        "outputId": "4ce762fa-bd15-402a-e0b6-41bc7be1f52d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "      <th>Content</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>It just works!</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>–í —Ü–µ–ª–æ–º —É–¥–æ–±–Ω–æ–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ...–∏–∑ –º–∏–Ω—É—Å–æ–≤ —Ö–æ—Ç—è...</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>–û—Ç–ª–∏—á–Ω–æ –≤—Å–µ</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>–°—Ç–∞–ª –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ 1% —Ä–∞–±–æ—Ç—ã –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–∞. –î–∞–ª—å—à–µ ...</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>–û—á–µ–Ω—å —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>–í—Å—ë —É–¥–æ–±–Ω–æ –Ω–æ—Ä–º üëçüëçüëç</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>–û—á–µ–Ω—å —É–¥–æ–±–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5</td>\n",
              "      <td>–í—Å–µ —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5</td>\n",
              "      <td>–£ –º–µ–Ω—è —Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ —á–µ—Ç–∫–æ. –í –æ—Ç–ª–∏—á–∏–∏ –æ—Ç –±–∞–Ω–∫–æ–º...</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>–û—á–µ–Ω—å –≤—Å–µ —Ö–æ—Ä–æ—à–æüëç</td>\n",
              "      <td>2017-08-14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rating                                            Content        Date\n",
              "0       5                                     It just works!  2017-08-14\n",
              "1       4  –í —Ü–µ–ª–æ–º —É–¥–æ–±–Ω–æ–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ...–∏–∑ –º–∏–Ω—É—Å–æ–≤ —Ö–æ—Ç—è...  2017-08-14\n",
              "2       5                                        –û—Ç–ª–∏—á–Ω–æ –≤—Å–µ  2017-08-14\n",
              "3       5  –°—Ç–∞–ª –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ 1% —Ä–∞–±–æ—Ç—ã –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–∞. –î–∞–ª—å—à–µ ...  2017-08-14\n",
              "4       5                     –û—á–µ–Ω—å —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.  2017-08-14\n",
              "5       5                                –í—Å—ë —É–¥–æ–±–Ω–æ –Ω–æ—Ä–º üëçüëçüëç  2017-08-14\n",
              "6       5                          –û—á–µ–Ω—å —É–¥–æ–±–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.  2017-08-14\n",
              "7       5                                     –í—Å–µ —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç  2017-08-14\n",
              "8       5  –£ –º–µ–Ω—è —Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ —á–µ—Ç–∫–æ. –í –æ—Ç–ª–∏—á–∏–∏ –æ—Ç –±–∞–Ω–∫–æ–º...  2017-08-14\n",
              "9       5                                  –û—á–µ–Ω—å –≤—Å–µ —Ö–æ—Ä–æ—à–æüëç  2017-08-14"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "df = pd.read_excel('./lection7/–æ—Ç–∑—ã–≤—ã –∑–∞ –ª–µ—Ç–æ.xls')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f025a75",
      "metadata": {
        "id": "2f025a75",
        "outputId": "cc26fc83-3148-4c66-b71d-3ce193e4776c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Rating\n",
              "5    14586\n",
              "1     2276\n",
              "4     2138\n",
              "3      911\n",
              "2      748\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Rating'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd27834c",
      "metadata": {
        "id": "dd27834c",
        "outputId": "26de5731-f1f8-4edd-8c06-cf676ccce0b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((13841, 3), (6818, 3))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.33, random_state=42)\n",
        "\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "df_train.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98dab30b",
      "metadata": {
        "id": "98dab30b",
        "outputId": "9af89674-4879-40d5-f900-0a87b5ae948f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "      <th>Content</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>–ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ –∏—Å–ø—Ä–∞–≤–∏–ª–∏ —ç—Ç—É —á—É—à—å —Å –Ω–µ–æ—Ä–≥–∏–Ω–∞–ª—å–Ω–æ–π ...</td>\n",
              "      <td>2017-08-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>–£–¥–æ–±–Ω–æ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏</td>\n",
              "      <td>2017-07-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>–û—Ç–ª–∏—á–Ω–æ</td>\n",
              "      <td>2017-08-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>–ö–ª–∞—Å—Å</td>\n",
              "      <td>2017-07-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>–£–¥–æ–±–Ω–æ</td>\n",
              "      <td>2017-07-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13836</th>\n",
              "      <td>4</td>\n",
              "      <td>–í—Å–µ –Ω—Ä–∞–≤–∏—Ç—Å—è</td>\n",
              "      <td>2017-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13837</th>\n",
              "      <td>5</td>\n",
              "      <td>–û—á–µ–Ω—å —Å–º–µ—à–Ω–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ø—É–≥–∞–µ—Ç—Å—è —Ä—É—Ç–∞ :)</td>\n",
              "      <td>2017-07-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13838</th>\n",
              "      <td>1</td>\n",
              "      <td>–ù–µ –º–æ–≥—É —Å–∫–∞—á–∞—Ç—å –æ—à–∏–±–∫–∞ –Ω–æ–º–µ—Ä 24</td>\n",
              "      <td>2017-08-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13839</th>\n",
              "      <td>5</td>\n",
              "      <td>–°–±–µ—Ä–±–∞–Ω–∫ –≤—Å–µ–≥–¥–∞ —Ä—è–¥–æ–º</td>\n",
              "      <td>2017-08-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13840</th>\n",
              "      <td>5</td>\n",
              "      <td>–í —Ü–µ–ª–æ–º –≤—Å–µ –æ—á–µ–Ω—å –æ—Ç–ª–∏—á–Ω–æ!)</td>\n",
              "      <td>2017-07-21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13841 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rating                                            Content        Date\n",
              "0           5  –ù–∞–∫–æ–Ω–µ—Ü-—Ç–æ –∏—Å–ø—Ä–∞–≤–∏–ª–∏ —ç—Ç—É —á—É—à—å —Å –Ω–µ–æ—Ä–≥–∏–Ω–∞–ª—å–Ω–æ–π ...  2017-08-09\n",
              "1           5                             –£–¥–æ–±–Ω–æ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏  2017-07-27\n",
              "2           5                                            –û—Ç–ª–∏—á–Ω–æ  2017-08-08\n",
              "3           5                                              –ö–ª–∞—Å—Å  2017-07-25\n",
              "4           5                                             –£–¥–æ–±–Ω–æ  2017-07-08\n",
              "...       ...                                                ...         ...\n",
              "13836       4                                       –í—Å–µ –Ω—Ä–∞–≤–∏—Ç—Å—è  2017-07-29\n",
              "13837       5            –û—á–µ–Ω—å —Å–º–µ—à–Ω–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ø—É–≥–∞–µ—Ç—Å—è —Ä—É—Ç–∞ :)  2017-07-28\n",
              "13838       1                    –ù–µ –º–æ–≥—É —Å–∫–∞—á–∞—Ç—å –æ—à–∏–±–∫–∞ –Ω–æ–º–µ—Ä 24  2017-08-06\n",
              "13839       5                              –°–±–µ—Ä–±–∞–Ω–∫ –≤—Å–µ–≥–¥–∞ —Ä—è–¥–æ–º  2017-08-12\n",
              "13840       5                        –í —Ü–µ–ª–æ–º –≤—Å–µ –æ—á–µ–Ω—å –æ—Ç–ª–∏—á–Ω–æ!)  2017-07-21\n",
              "\n",
              "[13841 rows x 3 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3bbfe5b",
      "metadata": {
        "id": "d3bbfe5b"
      },
      "outputs": [],
      "source": [
        "sw = set(get_stop_words(\"ru\"))\n",
        "exclude = set(punctuation)\n",
        "morpher = MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(txt):\n",
        "    txt = str(txt)\n",
        "    txt = \"\".join(c for c in txt if c not in exclude)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(\"\\s–Ω–µ\", \"–Ω–µ\", txt)\n",
        "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
        "    return \" \".join(txt)\n",
        "\n",
        "df_train['Content'] = df_train['Content'].apply(preprocess_text)\n",
        "df_test['Content'] = df_test['Content'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1305a23",
      "metadata": {
        "id": "d1305a23"
      },
      "outputs": [],
      "source": [
        "# Creating dictionary\n",
        "train_corpus = \" \".join(df_train[\"Content\"])\n",
        "train_corpus = train_corpus.lower()\n",
        "tokens = word_tokenize(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6212ed22",
      "metadata": {
        "id": "6212ed22",
        "outputId": "084b431f-821a-4592-a782-7715f97d0da2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': 4123, '—É–¥–æ–±–Ω–æ': 2201, '—Ä–∞–±–æ—Ç–∞—Ç—å': 1288, '—É–¥–æ–±–Ω—ã–π': 1182, '–æ—Ç–ª–∏—á–Ω–æ': 860, '–Ω—Ä–∞–≤–∏—Ç—å—Å—è': 763, '—Ö–æ—Ä–æ—à–∏–π': 681, '–æ—Ç–ª–∏—á–Ω—ã–π': 677, '—Ç–µ–ª–µ—Ñ–æ–Ω': 627, '—Å—É–ø–µ—Ä': 540, ...})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# filter and take only top N —Ç–æ–∫–µ–Ω–æ–≤\n",
        "tokens_filtered = [word for word in tokens if word.isalnum()] #filter to keep only letters and numbers\n",
        "\n",
        "# create token dictionary: number in corpus\n",
        "dist = FreqDist(tokens_filtered)\n",
        "dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7154dcb",
      "metadata": {
        "id": "c7154dcb",
        "outputId": "324ede4e-d80b-449d-cea8-cf546245dc55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ',\n",
              " '—É–¥–æ–±–Ω–æ',\n",
              " '—Ä–∞–±–æ—Ç–∞—Ç—å',\n",
              " '—É–¥–æ–±–Ω—ã–π',\n",
              " '–æ—Ç–ª–∏—á–Ω–æ',\n",
              " '–Ω—Ä–∞–≤–∏—Ç—å—Å—è',\n",
              " '—Ö–æ—Ä–æ—à–∏–π',\n",
              " '–æ—Ç–ª–∏—á–Ω—ã–π',\n",
              " '—Ç–µ–ª–µ—Ñ–æ–Ω',\n",
              " '—Å—É–ø–µ—Ä']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_words = 200\n",
        "\n",
        "\n",
        "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]\n",
        "tokens_filtered_top[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f451a7",
      "metadata": {
        "id": "32f451a7"
      },
      "outputs": [],
      "source": [
        "# create a vocabulary from top-200 tokens \n",
        "\n",
        "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289f8903",
      "metadata": {
        "id": "289f8903"
      },
      "outputs": [],
      "source": [
        "max_len = 40\n",
        "\n",
        "# preprocess train and test datasets \n",
        "# to lower register, tokenize, remove non number and non letter symbols\n",
        "# Check if token is in dictionary - add token's number to the result\n",
        "#otherwise - miss it and add zeros till the max_len\n",
        "\n",
        "def text_to_sequence(text, maxlen):\n",
        "    result = []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
        "    for word in tokens_filtered:\n",
        "        if word in vocabulary:\n",
        "            result.append(vocabulary[word])\n",
        "    padding = [0]*(maxlen-len(result))\n",
        "    return padding + result[-maxlen:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecbb1803",
      "metadata": {
        "id": "ecbb1803",
        "outputId": "7d4d9965-f42e-4670-d310-4bb39a5c0207"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   1,   2,  15],\n",
              "       [  0,   0,   0, ...,   0,   2, 181],\n",
              "       [  0,   0,   0, ...,   0,   0,   5],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 164,  27,  84],\n",
              "       [  0,   0,   0, ...,   0,   0,  20],\n",
              "       [  0,   0,   0, ...,   0, 113,   5]], dtype=int32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"Content\"]], dtype=np.int32)\n",
        "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"Content\"]], dtype=np.int32)\n",
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8a4b21",
      "metadata": {
        "id": "7d8a4b21",
        "outputId": "4fbb536d-6a71-4599-c888-a423dbf77bf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13841, 40)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141b44ba",
      "metadata": {
        "id": "141b44ba",
        "outputId": "c782f476-7d33-46fc-e079-a8ed1bf8d876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 5])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#one-hot encoding\n",
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(df_train['Rating']) \n",
        "test_enc_labels = le.transform(df_test['Rating'])\n",
        "le.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66c9522",
      "metadata": {
        "id": "c66c9522",
        "outputId": "00973c50-7652-43a1-c240-59031888f0d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 4, 4, ..., 0, 4, 4])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_enc_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab46288",
      "metadata": {
        "id": "8ab46288",
        "outputId": "fb0e8e1d-9f61-4663-d4a7-33d4c4b39089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = 5\n",
        "y_train = tf.keras.utils.to_categorical(train_enc_labels, num_classes=num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(test_enc_labels, num_classes=num_classes)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193ea7bc",
      "metadata": {
        "id": "193ea7bc"
      },
      "outputs": [],
      "source": [
        "## Keras  CONV model with preset Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=300, input_length=max_len))\n",
        "model.add(Conv1D(72, 3))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(GlobalMaxPool1D()) #choose the right pooling or you can use dense layer or rnn instead if accuracy is low\n",
        "model.add(Dense(10))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b40ea8",
      "metadata": {
        "id": "b3b40ea8"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=[get_f1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c071e442",
      "metadata": {
        "id": "c071e442"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ca033c",
      "metadata": {
        "id": "63ca033c"
      },
      "outputs": [],
      "source": [
        "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
        "early_stopping=EarlyStopping(monitor='val_loss') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc273ad",
      "metadata": {
        "id": "5cc273ad",
        "outputId": "5562a3aa-1626-4ef8-d6c4-e242e1c2e71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.5222 - get_f1: 0.0000e+00 - val_loss: 1.5020 - val_get_f1: 0.0000e+00\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.4836 - get_f1: 0.0000e+00 - val_loss: 1.4637 - val_get_f1: 0.0000e+00\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 1.4450 - get_f1: 0.0000e+00 - val_loss: 1.4248 - val_get_f1: 0.0000e+00\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.4054 - get_f1: 0.0000e+00 - val_loss: 1.3846 - val_get_f1: 0.0000e+00\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.3644 - get_f1: 0.0000e+00 - val_loss: 1.3429 - val_get_f1: 0.0000e+00\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.3217 - get_f1: 0.0000e+00 - val_loss: 1.2997 - val_get_f1: 0.0000e+00\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.2782 - get_f1: 0.0000e+00 - val_loss: 1.2565 - val_get_f1: 0.0000e+00\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.2349 - get_f1: 0.0000e+00 - val_loss: 1.2140 - val_get_f1: 0.0000e+00\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.1934 - get_f1: 0.0000e+00 - val_loss: 1.1739 - val_get_f1: 0.0000e+00\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.1551 - get_f1: 0.0000e+00 - val_loss: 1.1382 - val_get_f1: 0.0000e+00\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.1213 - get_f1: 0.0569 - val_loss: 1.1075 - val_get_f1: 0.7005\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.0927 - get_f1: 0.7021 - val_loss: 1.0822 - val_get_f1: 0.7005\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.0697 - get_f1: 0.7037 - val_loss: 1.0624 - val_get_f1: 0.7005\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.0514 - get_f1: 0.7040 - val_loss: 1.0472 - val_get_f1: 0.7005\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.0373 - get_f1: 0.7030 - val_loss: 1.0355 - val_get_f1: 0.7005\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 1.0267 - get_f1: 0.7033 - val_loss: 1.0267 - val_get_f1: 0.7005\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.0184 - get_f1: 0.7022 - val_loss: 1.0200 - val_get_f1: 0.7005\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 1.0119 - get_f1: 0.7051 - val_loss: 1.0151 - val_get_f1: 0.7005\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 1.0069 - get_f1: 0.7048 - val_loss: 1.0112 - val_get_f1: 0.7005\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 1.0029 - get_f1: 0.7038 - val_loss: 1.0082 - val_get_f1: 0.7005\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[tensorboard, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b31e861",
      "metadata": {
        "id": "6b31e861",
        "outputId": "3679300b-8ab8-4e89-9daf-0bf2ce56f7ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 13ms/step - loss: 0.9826 - get_f1: 0.7151\n",
            "\n",
            "\n",
            "Test loss: 0.9826066493988037\n",
            "Test f1_score: 0.7151089310646057\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('\\n')\n",
        "print('Test loss:', score[0])\n",
        "print('Test f1_score:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f603f27",
      "metadata": {
        "id": "1f603f27"
      },
      "outputs": [],
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353fa769",
      "metadata": {
        "id": "353fa769"
      },
      "outputs": [],
      "source": [
        "#!unzip glove*.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70da53fc",
      "metadata": {
        "id": "70da53fc",
        "outputId": "3b9cd9b9-3d1c-4f93-dfed-f02822714714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180.zip\n",
            "180.zip.1\n",
            "NLP_DF_HW7.ipynb\n",
            "NLP_DF_HW7_1.ipynb\n",
            "README\n",
            "glove.6B.100d.txt\n",
            "glove.6B.200d.txt\n",
            "glove.6B.300d.txt\n",
            "glove.6B.50d.txt\n",
            "glove.6B.zip\n",
            "glove.6B.zip.1\n",
            "hw7.txt\n",
            "\u001b[34mlection7\u001b[m\u001b[m\n",
            "lection7.zip\n",
            "\u001b[34mlogs\u001b[m\u001b[m\n",
            "meta.json\n",
            "model.bin\n",
            "model.txt\n",
            "–ü—Ä–∏–º–µ—Ä_—Ä–∞–∑–±–æ—Ä–∞_–î–ó_NLP_07_1.ipynb\n",
            "/Users/evamelissatasdemir/NLP/Lesson7\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a16c2f",
      "metadata": {
        "id": "87a16c2f"
      },
      "outputs": [],
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7773c35e",
      "metadata": {
        "id": "7773c35e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# embeddings_index = {}\n",
        "# f = open('glove.6B.100d.txt')\n",
        "# for line in f:\n",
        "#     values = line.split() # seperates each line by a white space\n",
        "#     word = values[0]  # the first element is the word\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')  # the rest are the weights\n",
        "#     # storing in dictionary\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9cf19a",
      "metadata": {
        "id": "fb9cf19a"
      },
      "outputs": [],
      "source": [
        "embeddings_dictionary = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06379615",
      "metadata": {
        "id": "06379615",
        "outputId": "5168ae36-1d06-4d14-fd55-5d47bcd1ca57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "glove_file = open('./glove.6B.300d.txt', 'rb')\n",
        "for line in glove_file:\n",
        "    records = line.split()  # seperates each line by a white space\n",
        "    word = records[0]  # the first element is the word\n",
        "    vector_dimensions = np.asarray(\n",
        "        records[1:], dtype='float32')  # the rest are the weights\n",
        "    # storing in dictionary\n",
        "    embeddings_dictionary[word] = vector_dimensions\n",
        "    \n",
        "glove_file.close()\n",
        "print('Found %s word vectors.' % len(embeddings_dictionary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc05fb3",
      "metadata": {
        "id": "2dc05fb3"
      },
      "outputs": [],
      "source": [
        "# len_of_vocab = len(word_index)\n",
        "embeddings_matrix = np.zeros((max_words , 300))\n",
        "# mapping to a new matrix, using only the words in your tokenizer's vocabulary\n",
        "for word, index in word_index.items():\n",
        "    if index>=max_words:\n",
        "        continue\n",
        "    # the weights of the individual words in your vocabulary\n",
        "    embedding_vector = embeddings_dictionary.get(bytes(word, 'utf-8'))\n",
        " \n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[index] = embedding_vector\n",
        "        #embeddings_matrix[index] = embedding_vector\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d4c6ee0",
      "metadata": {
        "id": "5d4c6ee0"
      },
      "outputs": [],
      "source": [
        "initializer = tf.keras.initializers.Constant(embeddings_matrix[:200])\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(input_dim=max_words, output_dim=300,\n",
        "          input_length=max_len, weights=[embeddings_matrix], embeddings_initializer =initializer, trainable=True))\n",
        "#model.add(Embedding(input_dim=max_words, output_dim=128, embeddings_initializer =initializer, input_length=max_len, trainable=False))\n",
        "model1.add(Conv1D(128, 3))\n",
        "model1.add(Activation(\"relu\"))\n",
        "model1.add(GlobalMaxPool1D())\n",
        "model1.add(Dense(10))\n",
        "model1.add(Activation(\"relu\"))\n",
        "model1.add(Dense(num_classes))\n",
        "model1.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dafc242",
      "metadata": {
        "id": "7dafc242",
        "outputId": "2ab5b3f4-e98a-4c61-9757-e39875769748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "217/217 [==============================] - 4s 19ms/step - loss: 0.7679 - get_f1: 0.7211\n",
            "Epoch 2/3\n",
            "217/217 [==============================] - 3s 15ms/step - loss: 0.6666 - get_f1: 0.7713\n",
            "Epoch 3/3\n",
            "217/217 [==============================] - 3s 16ms/step - loss: 0.6351 - get_f1: 0.7838\n",
            "f1 score: 76.59%\n"
          ]
        }
      ],
      "source": [
        "# Log to tensorboard\n",
        "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=[get_f1])\n",
        "\n",
        "\n",
        "model1.fit(x_train, y_train, epochs=3, callbacks=[\n",
        "          tensorBoardCallback], batch_size=64)\n",
        "\n",
        "\n",
        "# Evaluation on the test set\n",
        "scores = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"f1 score: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031df442",
      "metadata": {
        "id": "031df442",
        "outputId": "fabcf461-38f3-4bfa-bc7b-c33de4db43d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 14ms/step - loss: 0.6733 - get_f1: 0.7738\n",
            "\n",
            "\n",
            "Test loss: 0.6732770204544067\n",
            "Test f1: 0.7706745266914368\n"
          ]
        }
      ],
      "source": [
        "score = model1.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('\\n')\n",
        "print('Test loss:', scores[0])\n",
        "print('Test f1:', scores[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b1b0db",
      "metadata": {
        "id": "80b1b0db"
      },
      "outputs": [],
      "source": [
        "# Conclusion \n",
        "\n",
        "# I achieved better metrics using pre-trained word embeddings in a Keras model (f1_score = 0.77067)\n",
        "# 3) than Keras embadding (f1_score: 0.7151). Loss is less using glove pre-trained word embeddings(Test loss: 0.67327)\n",
        "# than Keras embadding(Test loss:0.9826)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d745d950",
      "metadata": {
        "id": "d745d950"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a91d6e6",
      "metadata": {
        "id": "4a91d6e6"
      },
      "outputs": [],
      "source": [
        "#import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f9e6e2",
      "metadata": {
        "id": "64f9e6e2"
      },
      "outputs": [],
      "source": [
        "#from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d518524",
      "metadata": {
        "id": "7d518524"
      },
      "outputs": [],
      "source": [
        "#model=gensim.models.Word2Vec(sentences) with model=Word2Vec(sentences)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}