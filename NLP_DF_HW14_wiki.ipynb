{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfridland/NLP/blob/HW14/NLP_DF_HW14_wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da3728c6",
      "metadata": {
        "scrolled": true,
        "id": "da3728c6"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset('mvarma/medwiki', 'medwiki_hq')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4263f5c",
      "metadata": {
        "id": "b4263f5c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
        "no_deprecation_warning=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872ed151",
      "metadata": {
        "scrolled": true,
        "id": "872ed151"
      },
      "outputs": [],
      "source": [
        "#! unzip medwiki_hq.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdac4e80",
      "metadata": {
        "id": "fdac4e80"
      },
      "outputs": [],
      "source": [
        "# test_hq = []\n",
        "# with open('test_hq.jsonl', 'r') as json_file:\n",
        "#     json_list = list(json_file)\n",
        "# for json_str in json_list:\n",
        "#     test_hq.append(json.loads(json_str)[\"sentence\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc18a96c",
      "metadata": {
        "id": "cc18a96c"
      },
      "outputs": [],
      "source": [
        "# len(test_hq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6568d37",
      "metadata": {
        "id": "e6568d37"
      },
      "outputs": [],
      "source": [
        "# test_hq[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b81542ee",
      "metadata": {
        "id": "b81542ee"
      },
      "outputs": [],
      "source": [
        "# train_hq = []\n",
        "# with open('test_hq.jsonl', 'r') as json_file:\n",
        "#     json_list = list(json_file)\n",
        "# for json_str in json_list:\n",
        "#     test_hq.append(json.loads(json_str))#[\"description\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe0ff60",
      "metadata": {
        "id": "dbe0ff60"
      },
      "outputs": [],
      "source": [
        "# test_hq = test_hq[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909de3a8",
      "metadata": {
        "id": "909de3a8"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with open('dev_hq.jsonl', 'r') as json_file:\n",
        "    json_list = list(json_file)\n",
        "for json_str in json_list:\n",
        "    data.append(json.loads(json_str)[\"sentence\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dce3993",
      "metadata": {
        "id": "6dce3993",
        "outputId": "146bad12-0bdc-4a0b-fa75-8f4bab97a714"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "165941"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dedfcf9e",
      "metadata": {
        "id": "dedfcf9e",
        "outputId": "50bbe01d-df4a-485a-f8b7-9f23bfbbad7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In 1844 , the family moved to the newly organized Yamhill District and settled on a land claim near the present community of Carlton .',\n",
              " \"This episode was removed from rotation in the United States after the 1995 Oklahoma City bombing due to its bombing/terrorism plot , and was consequently never rerun on Toon Disney , even before Disney 's stricter censorship policies following the September 11 attacks .\",\n",
              " 'Among patients with Evans syndrome , the prevailing causes of death were bleeding , infections , and hematological cancer .',\n",
              " 'The series is set in Los Angeles , California , but is filmed in Atlanta , Georgia `` .',\n",
              " \"Marquinhos featured in friendly wins over Colombia and Ecuador in Miami , making Marquinhos 's first start against the latter .\"]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0167799b",
      "metadata": {
        "id": "0167799b"
      },
      "outputs": [],
      "source": [
        "#data = data[:50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6dd71be",
      "metadata": {
        "id": "f6dd71be"
      },
      "outputs": [],
      "source": [
        "def build_text_files(data_json, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    for texts in data_json:\n",
        "        summary = str(texts).strip()\n",
        "        # summary = re.sub(r'<.*?>', \" \", summary) # –£–±–∏—Ä–∞–µ–º –Ω–∏–∫–Ω–µ–π–º—ã\n",
        "        summary = re.sub(r\"\\s\", \" \", summary)\n",
        "        data += summary + \"  \"\n",
        "    f.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602d1f6c",
      "metadata": {
        "id": "602d1f6c"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(data, test_size=0.15)\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733cde29",
      "metadata": {
        "id": "733cde29",
        "outputId": "53086c02-6510-4ce6-c9f1-324f165c1d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset length: 141049\n",
            "Test dataset length: 24892\n"
          ]
        }
      ],
      "source": [
        "print(\"Train dataset length: \"+ str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a448220",
      "metadata": {
        "id": "2a448220"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ComCom/gpt2-small\")\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecaacba",
      "metadata": {
        "scrolled": true,
        "id": "4ecaacba",
        "outputId": "767bee49-4b37-4481-ed80-bd15ea8e1e8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evamelissatasdemir/miniforge3/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b3e7f61",
      "metadata": {
        "id": "8b3e7f61"
      },
      "outputs": [],
      "source": [
        "# from transformers import GPT2Tokenizer, GPT2Model\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# model = GPT2Model.from_pretrained('gpt2')\n",
        "# text = \"Replace me by any text you'd like.\"\n",
        "# encoded_input = tokenizer(text, return_tensors='pt')\n",
        "# output = model(**encoded_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8a7946",
      "metadata": {
        "id": "0d8a7946"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"ComCom/gpt2-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b747018c",
      "metadata": {
        "id": "b747018c"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt_med_bash\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808572a2",
      "metadata": {
        "id": "808572a2"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2af512",
      "metadata": {
        "id": "bb2af512",
        "outputId": "f78e811c-40be-49c6-a0b7-fdab0c660e60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evamelissatasdemir/miniforge3/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1641' max='1641' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1641/1641 10:44, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.577100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.669700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>6.451600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1641, training_loss=6.8534332777508, metrics={'train_runtime': 645.6501, 'train_samples_per_second': 10.166, 'train_steps_per_second': 2.542, 'total_flos': 428780224512000.0, 'train_loss': 6.8534332777508, 'epoch': 3.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc2bdb7e",
      "metadata": {
        "id": "dc2bdb7e"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef62c179",
      "metadata": {
        "id": "ef62c179"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained('gpt_med_bash')\n",
        "model.save_pretrained('model_gpt_med_bash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8512072",
      "metadata": {
        "id": "e8512072"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt_med_bash')\n",
        "model1 = AutoModelForCausalLM.from_pretrained('model_gpt_med_bash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f061b4",
      "metadata": {
        "id": "a6f061b4"
      },
      "outputs": [],
      "source": [
        "prefix = \"He won election to the final session \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcecbbaa",
      "metadata": {
        "id": "bcecbbaa"
      },
      "outputs": [],
      "source": [
        "def generate(prefix, gen_legth=50):\n",
        "\n",
        "    tokens = tokenizer(prefix, return_tensors='pt')\n",
        "\n",
        "    size = tokens['input_ids'].shape[1]\n",
        "    output = model1.generate(\n",
        "        **tokens,\n",
        "        #end_token=end_token_id,\n",
        "        do_sample=False,\n",
        "        max_length=size+gen_legth,\n",
        "        repetition_penalty=5.,\n",
        "        temperature=0.5,\n",
        "        num_beams=10,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0])\n",
        "    result = decoded[len(prefix):]\n",
        "    print(prefix + result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ba7747",
      "metadata": {
        "id": "21ba7747",
        "outputId": "db82f73f-a316-42a7-fb41-b3fd7e01f1e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He won election to the final session ÔøΩ pornWJcldcrete` mist sincebC H for howts1 relow thisD pornwJcldcrete` mist sincebC hckal Ad3 operated pornWJcldcrete` mist sincebC\n"
          ]
        }
      ],
      "source": [
        "generate(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e0c702",
      "metadata": {
        "id": "50e0c702",
        "outputId": "0dfe7a97-9dd3-45c2-ab2b-76d1928c5f63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I apply for jobÔøΩ pornWJcldcrete` mist sincebC Helow single1 rD pornwJcldcrete` mist sincebC hckalploy3 operated pornWJcldcrete` mist sincebC reports2 nual pornWJcldcrete` mist sincebC syally T plore' pornWJcldcrete` mist sincebC scol F will u this an D sceneTABLE pornWJcldcrete` mist sincebC\n"
          ]
        }
      ],
      "source": [
        "generate('How can I apply for job', gen_legth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c30f3049",
      "metadata": {
        "id": "c30f3049"
      },
      "outputs": [],
      "source": [
        "#checking if there are the same phrases in dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db2335f",
      "metadata": {
        "id": "6db2335f"
      },
      "outputs": [],
      "source": [
        "def find_sentence(filename, sentence):\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            stringA = line\n",
        "    match = re.search(sentence, stringA)\n",
        "\n",
        "    if match:\n",
        "        print('Yes!')\n",
        "    else:\n",
        "        print('No!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f695f5",
      "metadata": {
        "id": "36f695f5",
        "outputId": "7263c760-41dd-4c06-a451-8df15c049e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No!\n",
            "No!\n",
            "No!\n",
            "Yes!\n",
            "No!\n"
          ]
        }
      ],
      "source": [
        "find_sentence('./train_dataset.txt', 'He had run for the office of U.S. Marshall for the territory.')\n",
        "find_sentence('./test_dataset.txt', 'He had run for the office of U.S. Marshall for the territory')\n",
        "find_sentence('./train_dataset.txt', '<REal_SM[techsupport]>')\n",
        "find_sentence('./train_dataset.txt', \"Marquinhos featured in friendly wins over Colombia and Ecuador in Miami , making Marquinhos 's first start against the latter.\")\n",
        "# –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Ñ—Ä–∞–∑–∞ –µ—Å—Ç—å –≤ —Ç—Ä–µ–π–Ω–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
        "find_sentence('./test_dataset.txt', \"Marquinhos featured in friendly wins over Colombia and Ecuador in Miami , making Marquinhos 's first start against the latter.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}