{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfridland/NLP/blob/HW11/NLP_DF_HW11_transformer_actual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Transformer model for language understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-f8TnGpE_ex"
      },
      "source": [
        "Основная идея, лежащая в основе модели Transformer, - это самовнимание - способность отслеживать различные позиции входной последовательности для вычисления представления этой последовательности. Transformer создает стопки слоев самовнимания, что объясняется ниже в разделах « Масштабируемое скалярное произведение» и « Многоголовое внимание» .\n",
        "\n",
        "Модель преобразователя обрабатывает входные данные переменного размера, используя стопки слоев самовнимания вместо RNN или CNN . Эта общая архитектура имеет ряд преимуществ:\n",
        "\n",
        "* Он не делает никаких предположений о временных / пространственных отношениях между данными. Это идеально подходит для обработки набора объектов.\n",
        "* Выходы уровня можно вычислять параллельно, а не последовательно, как RNN.\n",
        "* Отдаленные элементы могут влиять на вывод друг друга, не проходя через множество RNN-шагов или слоев свертки (например, см. Преобразователь памяти сцены ).\n",
        "* Он может изучать дальнодействующие зависимости. Это проблема для многих задач последовательности.\n",
        "\n",
        "Минусы этой архитектуры:\n",
        "\n",
        "* Для временного ряда выход для временного шага рассчитывается из всей истории, а не только из входных данных и текущего скрытого состояния. Это может быть менее эффективно.\n",
        "* Если вход действительно имеет временное / пространственное соотношение, как текст, должно быть добавлено некоторое позиционное кодирование или модель будет эффективно увидеть мешок слов.\n",
        "\n",
        "После обучения модели вы сможете ввести предложение на русском и вернуть перевод на английском.\n",
        "\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" width=\"800\" alt=\"Attention heatmap\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "XFG0NDRu5mYQ",
        "outputId": "fe33ba1f-e44e-4ecf-a6f2-d99579b232c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m4.2/5.4 MB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.2.2\n",
            "  Downloading matplotlib-3.2.2.tar.gz (40.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (1.22.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.16.0)\n",
            "Building wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-3.2.2-cp310-cp310-linux_x86_64.whl size=12292142 sha256=41747323935b3a272284e4e9ce8ba84cf9ff3542e61ce1a4a112cc341163444f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/81/f3/48b8bd245846ae69fcb2281c84e848bfea1f5260a870c148ae\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mizani 0.8.1 requires matplotlib>=3.5.0, but you have matplotlib 3.2.2 which is incompatible.\n",
            "plotnine 0.10.1 requires matplotlib>=3.5.0, but you have matplotlib 3.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#!pip install -q tfds-nightly\n",
        "#\n",
        "# Pin matplotlib version to 3.2.2 since in the latest version\n",
        "# transformer.ipynb fails with the following error:\n",
        "# https://stackoverflow.com/questions/62953704/valueerror-the-number-of-fixedlocator-locations-5-usually-from-a-call-to-set\n",
        "!pip install matplotlib==3.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UvmxWj_yOOhT"
      },
      "outputs": [],
      "source": [
        "#!pip install notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dm6O-hyvOOhT"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm as notebook_tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlk0EoL-OOhU",
        "outputId": "60595652-e493-4017-f4a1-2b0b77f8ff12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-628a32c4b606>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
        "is_cuda_gpu_available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4_Qt8W1hJE_"
      },
      "source": [
        "Use [TFDS](https://www.tensorflow.org/datasets) to load the Ru-English translation dataset from the [TED Talks Open Translation Project](https://www.ted.com/participate/translate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NC79q68oOOhU"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SSHIXaLAOOhU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KVBg5Q8tBk5z"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4DYWukNFkGQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a58d4f-4c47-4bd2-8a12-d12be9b96696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized string is [8073, 1034, 8104, 5774, 13, 3531, 8035]\n",
            "The original string: Transformer is awesome.\n"
          ]
        }
      ],
      "source": [
        "sample_string = 'Transformer is awesome.'\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_en.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9KJWJjrsZ4Y"
      },
      "source": [
        "Токенизатор кодирует строку, разбивая ее на подслова, если слово отсутствует в его словаре.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bf2ntBxjkqK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5ef05f-c4a4-4804-c473-dbef16ff6850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8073 ----> T\n",
            "1034 ----> ran\n",
            "8104 ----> s\n",
            "5774 ----> former \n",
            "13 ----> is \n",
            "3531 ----> awesome\n",
            "8035 ----> .\n"
          ]
        }
      ],
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bcRp7VcQ5m6g"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGi4PoVakxdc"
      },
      "source": [
        "Добавьте начальный и конечный токены к входу и цели.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFrcjGryOOhW"
      },
      "source": [
        "The `encode` function takes two language sentences, `lang1` and `lang2`, and encodes them using the respective tokenizers (`tokenizer_pt` for the Portuguese language and `tokenizer_en` for the English language). It performs the following steps:\n",
        "\n",
        "1. It converts the `lang1` sentence to a numpy array using the `.numpy()` method.\n",
        "2. It adds the start and end tokens to the `lang1` sentence by appending the vocabulary size and vocabulary size plus one tokens from the Portuguese tokenizer.\n",
        "3. It performs the same steps for the `lang2` sentence, using the English tokenizer.\n",
        "\n",
        "Finally, it returns the encoded `lang1` and `lang2` sentences as lists.\n",
        "\n",
        "Note: The use of `.numpy()` suggests that `lang1` and `lang2` are TensorFlow tensors, and the conversion to numpy arrays is necessary to work with the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UZwnPr4R055s"
      },
      "outputs": [],
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
        "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "  \n",
        "  return lang1, lang2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx1sFbR-9fRs"
      },
      "source": [
        "Вы хотите использовать Dataset.map чтобы применить эту функцию к каждому элементу набора данных. Dataset.map работает в графическом режиме.\n",
        "\n",
        "Тензоры графов не имеют значения.\n",
        "В графическом режиме вы можете использовать только операции и функции TensorFlow.\n",
        "Таким образом, вы не можете напрямую .map эту функцию: вам нужно обернуть ее в tf.py_function . tf.py_function будет передавать обычные тензоры (со значением и .numpy() для доступа к нему) в .numpy() функцию python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1A0Klb4OOhX"
      },
      "source": [
        "The tf_encode function is a TensorFlow function that applies the encode function to pairs of sentences in parallel. It takes two arguments, pt and en, which represent batches of sentences in the Portuguese and English languages, respectively. The function performs the following steps:\n",
        "\n",
        "It uses tf.py_function to wrap the encode function, allowing it to be used within TensorFlow computational graphs.\n",
        "It passes the pt and en tensors to the encode function as arguments.\n",
        "It specifies the output types of the resulting tensors as tf.int64 using the result_pt and result_en tensors.\n",
        "It sets the shapes of the output tensors using set_shape([None]), indicating that the shape of the tensors can vary along the first dimension (batch size).\n",
        "The function then returns the encoded tensors result_pt and result_en.\n",
        "\n",
        "This function can be used as a preprocessing step when preparing data for training a sequence-to-sequence model. It allows encoding of batches of sentences in parallel using TensorFlow operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mah1cS-P70Iz"
      },
      "outputs": [],
      "source": [
        "def tf_encode(pt, en):\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "  result_pt.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "\n",
        "  return result_pt, result_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2QEgbjntk6Yf"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBIxFxhcOOhX"
      },
      "source": [
        "The `filter_max_length` function is a filtering function that takes in two sequences, `x` and `y`, and a maximum length limit `max_length`. It returns a boolean value indicating whether the lengths of both sequences are less than or equal to the specified maximum length.\n",
        "\n",
        "The function uses TensorFlow operations to compute the size of the sequences (`tf.size(x)` and `tf.size(y)`) and then applies a logical AND operation (`tf.logical_and`) to check if both sizes are within the maximum length limit.\n",
        "\n",
        "This function can be used to filter out sequences that exceed a certain length during data preprocessing or when creating a dataset. It ensures that sequences are truncated or omitted if they are too long, which can be useful for managing computational resources or ensuring compatibility with models that have fixed input length requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c081xPGv1CPI"
      },
      "outputs": [],
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDe2UomOOhY"
      },
      "source": [
        "The code snippet provided demonstrates the processing steps for creating the training and validation datasets:\n",
        "\n",
        "1. `train_examples` and `val_examples` are assumed to be the input examples for the training and validation sets, respectively.\n",
        "2. The `tf_encode` function is applied to each example in `train_examples` and `val_examples` using the `map` function. This function converts the input sentences from the source language (e.g., Portuguese) and target language (e.g., English) into tokenized sequences represented as integer tensors.\n",
        "3. The `filter_max_length` function is then applied to both `train_dataset` and `val_dataset` to filter out sequences that exceed the maximum length specified by `MAX_LENGTH`.\n",
        "4. The `train_dataset` is cached in memory using the `cache` method. Caching the dataset helps to speed up the data reading process.\n",
        "5. The `train_dataset` is shuffled with a buffer size specified by `BUFFER_SIZE` and then batched into batches of size `BATCH_SIZE` using the `shuffle` and `padded_batch` methods, respectively. Shuffling the dataset ensures randomness during training, and padding the batches ensures that all sequences within a batch have the same length.\n",
        "6. The `val_dataset` is also processed similarly, but without shuffling. It is only padded into batches using the `padded_batch` method.\n",
        "\n",
        "Finally, the datasets are ready for training and validation, with the training dataset benefiting from caching, shuffling, and batching operations, and the validation dataset being batched for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9mk9AZdZ5bcS"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_fXvfYVfQr2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c96bb90-ac13-4c06-e2cb-a9a1f31dadc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
              " array([[8179,   57,   86, ...,    0,    0,    0],\n",
              "        [8179,    3,   38, ...,    0,    0,    0],\n",
              "        [8179,   57,  135, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8179,    3,    7, ...,    0,    0,    0],\n",
              "        [8179,  138,  250, ...,    0,    0,    0],\n",
              "        [8179,   19,    7, ...,    0,    0,    0]])>,\n",
              " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
              " array([[8245,   90,  101, ...,    0,    0,    0],\n",
              "        [8245,   70,   25, ...,    0,    0,    0],\n",
              "        [8245,   90,  153, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8245,    4,   18, ...,    0,    0,    0],\n",
              "        [8245,   19,   59, ...,    0,    0,    0],\n",
              "        [8245,   24,   18, ...,    0,    0,    0]])>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\n",
        "pt_batch, en_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBQuibYA4n0n"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "Поскольку эта модель не содержит повторений или сверток, добавляется позиционное кодирование, чтобы дать модели некоторую информацию об относительном положении слов в предложении.\n",
        "\n",
        "Вектор позиционного кодирования добавляется к вектору внедрения. Вложения представляют собой токен в d-мерном пространстве, где токены с одинаковым значением будут ближе друг к другу. Но вложения не кодируют относительное положение слов в предложении. Таким образом, после добавления позиционной кодировки слова будут ближе друг к другу на основе сходства их значения и их положения в предложении в d-мерном пространстве.\n",
        "\n",
        "Формула для расчета позиционного кодирования выглядит следующим образом:\n",
        "\n",
        "\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDruNHicOOhY"
      },
      "source": [
        "The `get_angles` function calculates the angle rates for positional encoding in a Transformer model. It takes three arguments:\n",
        "\n",
        "1. `pos`: A tensor representing the positions of the elements in a sequence. It has shape `(length,)`, where `length` is the length of the sequence.\n",
        "2. `i`: An integer representing the dimension index.\n",
        "3. `d_model`: An integer representing the dimensionality of the model.\n",
        "\n",
        "The function calculates the angle rates using the formula `1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))`. This formula introduces a scaling factor based on the position and the dimension index to determine the frequency of the sine and cosine functions used for positional encoding.\n",
        "\n",
        "The `pos` tensor is multiplied element-wise with the angle rates to obtain the final angles. The resulting tensor has the same shape as the `pos` tensor.\n",
        "\n",
        "The purpose of calculating these angles is to provide a unique positional encoding for each element in the sequence, allowing the Transformer model to capture positional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WhIOZjMNKujn"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp3RPbjeOOhZ"
      },
      "source": [
        "The `positional_encoding` function generates positional encodings for a given sequence length (`position`) and model dimensionality (`d_model`). It uses the `get_angles` function to calculate the angles based on the position and dimension.\n",
        "\n",
        "Here's how the function works:\n",
        "\n",
        "1. It first calls `get_angles` with the range of positions (`np.arange(position)[:, np.newaxis]`) and the range of dimensions (`np.arange(d_model)[np.newaxis, :]`), resulting in an `angle_rads` tensor of shape `(position, d_model)`.\n",
        "\n",
        "2. The `angle_rads` tensor is then modified by applying the `np.sin` function to even indices (`angle_rads[:, 0::2]`) and the `np.cos` function to odd indices (`angle_rads[:, 1::2]`). This ensures that even dimensions capture sine-based positional information, while odd dimensions capture cosine-based positional information.\n",
        "\n",
        "3. Finally, the `pos_encoding` tensor is created by adding an extra dimension to the modified `angle_rads` tensor (`angle_rads[np.newaxis, ...]`). This extra dimension represents the batch dimension and allows the positional encoding to be broadcasted across multiple sequences.\n",
        "\n",
        "4. The resulting `pos_encoding` tensor is cast to `tf.float32` data type and returned.\n",
        "\n",
        "The purpose of positional encoding is to provide the Transformer model with information about the relative positions of elements in the input sequence. It helps the model differentiate between different positions and encode sequential dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1Rz82wEs5biZ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlkMduY8OOhZ"
      },
      "source": [
        "The code snippet you provided generates a positional encoding matrix (`pos_encoding`) for a sequence length of 50 and a model dimensionality of 512. The `pos_encoding` matrix has a shape of `(1, 50, 512)`.\n",
        "\n",
        "The code then visualizes the positional encoding matrix using a color mesh plot. The x-axis represents the depth or dimension of the model, ranging from 0 to 512. The y-axis represents the position in the sequence, ranging from 0 to 49 (since Python indexing starts from 0).\n",
        "\n",
        "Each cell in the plot corresponds to the value of the positional encoding at a specific position and dimension. The color intensity represents the magnitude of the value, with a color map ranging from red to blue.\n",
        "\n",
        "The resulting plot provides a visual representation of how the positional encoding varies across different positions and dimensions. It shows how the encoding values change sinusoidally, alternating between sine and cosine patterns along the depth dimension.\n",
        "\n",
        "Here's the updated code with the visualization:\n",
        "\n",
        "```python\n",
        "pos_encoding = positional_encoding(50, 512)\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1kLCla68EloE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "d410d635-8c5d-42f5-8670-35cc299ba427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 50, 512)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42Uydjkv0hF"
      },
      "source": [
        "Замаскируйте все маркеры площадок в пакете последовательности. Это гарантирует, что модель не обрабатывает отступы как входные данные. Маска указывает, где присутствует значение пэда 0 : она выводит 1 в этих местах и 0 противном случае."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD6MroyVOOha"
      },
      "source": [
        "The `create_padding_mask` function you provided creates a padding mask for a given sequence (`seq`). It assumes that the padding token has a value of 0 in the sequence.\n",
        "\n",
        "Here's how the function works:\n",
        "\n",
        "1. It uses `tf.math.equal(seq, 0)` to create a tensor of the same shape as `seq`, where each element is `True` if the corresponding element in `seq` is equal to 0, and `False` otherwise.\n",
        "2. It casts the resulting boolean tensor to `tf.float32` to convert `True` values to 1.0 and `False` values to 0.0.\n",
        "3. It adds extra dimensions to the tensor to match the expected shape for the attention logits. The resulting tensor has a shape of `(batch_size, 1, 1, seq_len)`, where `batch_size` is the number of sequences in the batch, and `seq_len` is the length of each sequence.\n",
        "\n",
        "The padding mask is used to mask out the padding tokens during the self-attention computation in the transformer model. By assigning a large negative value (e.g., -1e9) to the padding positions in the attention logits, the softmax operation will effectively assign a probability of 0 to the padding tokens.\n",
        "\n",
        "Here's an example usage of the `create_padding_mask` function:\n",
        "\n",
        "```python\n",
        "seq = tf.constant([3, 0, 7, 0, 0, 2])\n",
        "padding_mask = create_padding_mask(seq)\n",
        "print(padding_mask)\n",
        "\n",
        "Output:\n",
        "[[[[0. 1. 0. 1. 1. 0.]]]]\n",
        "```\n",
        "\n",
        "In this example, the input sequence has a length of 6, and the padding tokens are located at positions 1, 3, 4. The resulting padding mask tensor has a shape of `(1, 1, 1, 6)` and indicates the positions of the padding tokens with a value of 1.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U2i8-e1s8ti9"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7BYeBCNvi7n"
      },
      "outputs": [],
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0hzukDBgVom"
      },
      "source": [
        "Маска упреждающего просмотра используется для маскировки будущих токенов в последовательности. Другими словами, маска указывает, какие записи не следует использовать.\n",
        "\n",
        "Это означает, что для предсказания третьего слова будут использоваться только первое и второе слово. Аналогично для предсказания четвертого слова будут использоваться только первое, второе и третье слово и так далее.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dVxS8OPI9uI0"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yxKGuXxaBeeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a6b94d-33b6-485f-f7db-309bca32071b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsxEE_-Wa1gF"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
        "\n",
        "Функция внимания, используемая преобразователем, принимает три входа: Q (запрос), K (ключ), V (значение). Уравнение, используемое для расчета весов внимания:\n",
        "\n",
        "\n",
        "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
        "\n",
        "Внимание скалярного произведения масштабируется с коэффициентом квадратного корня из глубины. Это сделано потому, что для больших значений глубины скалярное произведение увеличивается по величине, подталкивая функцию softmax, где у него есть небольшие градиенты, что приводит к очень жесткому softmax.\n",
        "\n",
        "Например, предположим, что Q и K имеют среднее значение 0 и дисперсию 1. Их матричное умножение будет иметь среднее значение 0 и дисперсию dk . Следовательно, для масштабирования используется квадратный корень из dk (а не какое-либо другое число), потому что матрица Q и K должна иметь среднее значение 0 и дисперсию 1, и вы получите более мягкий softmax.\n",
        "\n",
        "Маска умножается на -1e9 (близко к отрицательной бесконечности). Это сделано потому, что маска суммируется с умножением масштабированной матрицы Q и K и применяется непосредственно перед softmax. Цель состоит в том, чтобы обнулить эти ячейки, и большие отрицательные входные данные для softmax близки к нулю на выходе.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LazzUq3bJ5SH"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiqETnhCkoXh"
      },
      "source": [
        "Поскольку нормализация softmax выполняется для K, его значения определяют степень важности, придаваемой Q.\n",
        "\n",
        "Выходные данные представляют собой умножение весов внимания и вектора V (значения). Это гарантирует, что слова, на которых вы хотите сосредоточиться, останутся как есть, а нерелевантные слова будут удалены.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "n90YjClyInFy"
      },
      "outputs": [],
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yAzUAf2DPlNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c49c563-9893-4c13-faf2-d021b16286b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zg6k-fGhgXra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44404806-ed71-47af-fd04-71efbcd72db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# This query aligns with a repeated key (third and fourth), \n",
        "# so all associated values get averaged.\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UAq3YOzUgXhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3dc1a43-4c37-44da-b775-aeed15a90dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# This query aligns equally with the first and second key, \n",
        "# so their values get averaged.\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOz-4_XIhaTP"
      },
      "source": [
        "Pass all the queries together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6dlU8Tm-hYrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc398964-a0d4-414f-f35b-44e84832345c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor(\n",
            "[[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor(\n",
            "[[550.    5.5]\n",
            " [ 10.    0. ]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz5BMC8Kaoqo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
        "\n",
        "\n",
        "Многоголовое внимание состоит из четырех частей:\n",
        "\n",
        "* Слои линейные и разбиваются на головы.\n",
        "* Повышенное внимание к скалярному продукту\n",
        "* Конкатенация голов.\n",
        "* Финальный линейный слой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPmbr6F1C-v_"
      },
      "source": [
        "Каждый блок внимания с несколькими головами получает три входа; Q (запрос), K (ключ), V (значение). Они проходят через линейные (плотные) слои и разбиваются на несколько головок.\n",
        "\n",
        "scaled_dot_product_attention определенный выше, применяется к каждой голове (транслируется для эффективности). На этапе внимания необходимо использовать соответствующую маску. Затем вывод внимания для каждой головы объединяется (с использованием tf.transpose и tf.reshape ) и пропускается через последний слой Dense .\n",
        "\n",
        "Вместо одной единственной головы внимания Q, K и V разделены на несколько голов, потому что это позволяет модели совместно обращать внимание на информацию в разных положениях из разных пространств представления. После разделения каждая голова имеет уменьшенную размерность, поэтому общая стоимость вычислений такая же, как и внимание одной головы с полной размерностью.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BSV3PPKsYecw"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D8FJue5lDyZ"
      },
      "source": [
        "Создайте слой MultiHeadAttention чтобы попробовать. В каждом месте в последовательности y MultiHeadAttention запускает все 8 головок внимания по всем другим местам в последовательности, возвращая новый вектор той же длины в каждом месте.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Hu94p-_-2_BX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c53ca1-1180-4dff-ada8-59d5a35515b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdDqGayx67vv"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBqzJXGfHK3X"
      },
      "source": [
        "Сеть с точечной прямой связью состоит из двух полностью связанных слоев с активацией ReLU между ними.\n",
        "The code implements a point-wise feed-forward network using the TensorFlow library. Let's break it down:\n",
        "\n",
        "The point_wise_feed_forward_network function takes two arguments:\n",
        "\n",
        "d_model: This represents the input and output dimension of the feed-forward network, which is the model's hidden size or embedding dimension.\n",
        "dff: This specifies the hidden dimension of the feed-forward network, determining the size of the intermediate representation.\n",
        "The function returns a tf.keras.Sequential model, which is a linear stack of layers.\n",
        "\n",
        "The point-wise feed-forward network consists of two tf.keras.layers.Dense layers:\n",
        "\n",
        "The first dense layer has dff units and applies the ReLU activation function. It transforms the input tensor from shape (batch_size, seq_len, d_model) to shape (batch_size, seq_len, dff).\n",
        "The second dense layer has d_model units and does not have an activation function specified. It transforms the tensor back to the original input dimension, resulting in the output tensor shape (batch_size, seq_len, d_model).\n",
        "In summary, this code defines a point-wise feed-forward network with two dense layers in a sequential manner. The first dense layer transforms the input tensor to a higher-dimensional representation using ReLU activation, and the second dense layer maps it back to the original dimension without any activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ET7xLt0yCT6Z"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mytb1lPyOHLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbf7ccc-82fa-48af-ecf9-63dbccbf650b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yScbC0MUH8dS"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfYJG-Kvgwy2"
      },
      "source": [
        "Модель трансформера следует той же общей схеме, что и стандартная последовательность действий с моделью внимания .\n",
        "\n",
        "Входное предложение проходит через N уровней кодировщика, которые генерируют выходные данные для каждого слова / токена в последовательности.\n",
        "Декодер отслеживает вывод кодировщика и свой собственный ввод (самовнимание), чтобы предсказать следующее слово.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### Encoder layer\n",
        "\n",
        "Каждый уровень кодировщика состоит из подслоев:\n",
        "\n",
        "Многоголовое внимание (с дополнительной маской)\n",
        "Точечные сети прямого распространения.\n",
        "Каждый из этих подуровней имеет остаточную связь вокруг себя, за которой следует нормализация уровня. Остаточные соединения помогают избежать проблемы исчезающего градиента в глубоких сетях.\n",
        "\n",
        "Результатом каждого подслоя является LayerNorm(x + Sublayer(x)) . Нормализация выполняется по d_model (последняя). В трансформаторе N слоев кодировщика.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_7Fk-MOOhd"
      },
      "source": [
        "Sure! The code snippet you provided defines an `EncoderLayer` class in TensorFlow. Let's break it down step by step:\n",
        "\n",
        "1. The `EncoderLayer` class is derived from `tf.keras.layers.Layer`, indicating that it is a custom layer in TensorFlow.\n",
        "\n",
        "2. The `__init__` method is the constructor of the class. It takes four arguments:\n",
        "   - `d_model`: The input and output dimension of the encoder layer, representing the hidden size or embedding dimension.\n",
        "   - `num_heads`: The number of heads in the multi-head attention mechanism.\n",
        "   - `dff`: The dimension of the hidden layer in the point-wise feed-forward network.\n",
        "   - `rate`: The dropout rate, which is set to 0.1 by default.\n",
        "\n",
        "3. Inside the constructor, the following components are defined:\n",
        "   - `self.mha`: An instance of the `MultiHeadAttention` class, representing the multi-head attention mechanism. It takes `d_model` and `num_heads` as arguments.\n",
        "   - `self.ffn`: An instance of the `point_wise_feed_forward_network` function, representing the point-wise feed-forward network. It takes `d_model` and `dff` as arguments.\n",
        "   - `self.layernorm1` and `self.layernorm2`: Instances of `tf.keras.layers.LayerNormalization`, representing the layer normalization operations applied to the outputs.\n",
        "   - `self.dropout1` and `self.dropout2`: Instances of `tf.keras.layers.Dropout`, representing dropout layers for regularization.\n",
        "\n",
        "4. The `call` method defines the forward pass of the `EncoderLayer`:\n",
        "   - It takes three arguments: `x` (the input tensor), `training` (a boolean flag indicating whether the model is in training or inference mode), and `mask` (an optional mask tensor for masking padded positions).\n",
        "   - The multi-head attention mechanism (`self.mha`) is applied to the input `x` three times with the same inputs (`x`, `x`, `x`) and the `mask` to obtain `attn_output`, which represents the attention-weighted sum of the input.\n",
        "   - The `attn_output` is then passed through the first dropout layer (`self.dropout1`) with the specified dropout rate and the `training` flag.\n",
        "   - The original input `x` is added to the dropout output, and the result is normalized using `self.layernorm1`.\n",
        "   - The output of the layer normalization (`out1`) is passed through the point-wise feed-forward network (`self.ffn`) to obtain `ffn_output`.\n",
        "   - The `ffn_output` is then passed through the second dropout layer (`self.dropout2`) with the specified dropout rate and the `training` flag.\n",
        "   - The original input `x` is again added to the dropout output, and the result is normalized using `self.layernorm2`.\n",
        "   - The final output of the `EncoderLayer` is returned as `out2`.\n",
        "\n",
        "In summary, the `EncoderLayer` class represents a single layer in the encoder of a transformer model. It consists of a multi-head attention mechanism, a point-wise feed-forward network, layer normalization operations, and dropout layers for regularization. The `call` method defines the computations performed during the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ncyS-Ms3i2x_"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su9IYhkROOhe"
      },
      "source": [
        "The code snippet you provided demonstrates the usage of the `EncoderLayer` class. Let's go through it step by step:\n",
        "\n",
        "1. `sample_encoder_layer` is an instance of the `EncoderLayer` class created with the following arguments:\n",
        "   - `d_model = 512`: The input and output dimension of the encoder layer.\n",
        "   - `num_heads = 8`: The number of heads in the multi-head attention mechanism.\n",
        "   - `dff = 2048`: The dimension of the hidden layer in the point-wise feed-forward network.\n",
        "   - The `rate` argument is not provided, so it takes the default value of 0.1.\n",
        "\n",
        "2. `sample_encoder_layer_output` is obtained by calling `sample_encoder_layer` with the following arguments:\n",
        "   - `tf.random.uniform((64, 43, 512))`: A random input tensor of shape `(batch_size, input_seq_len, d_model)`. Here, the batch size is 64, the input sequence length is 43, and the hidden size (d_model) is 512.\n",
        "   - `False`: The `training` flag is set to `False`, indicating that the model is in inference mode.\n",
        "   - `None`: The `mask` argument is set to `None`, meaning no masking is applied.\n",
        "\n",
        "3. Finally, `sample_encoder_layer_output.shape` is used to print the shape of the output tensor. The expected shape is `(batch_size, input_seq_len, d_model)`, which represents the batch size, sequence length, and hidden size of the encoder layer output.\n",
        "\n",
        "By running this code, you will obtain the shape of the output tensor from the sample encoder layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "AzZRXdO0mI48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86abcf0-eae8-4e80-da52-25228eeea50b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "Каждый слой декодера состоит из подслоев:\n",
        "\n",
        "Замаскированное внимание с несколькими головами (с опережающей маской и дополнительной маской)\n",
        "Многоголовое внимание (с дополнительной маской). V (значение) и K (ключ) получают выходной сигнал энкодера в качестве входных данных. Q (запрос) получает выходные данные от подуровня замаскированного многоголового внимания.\n",
        "Точечные сети прямого распространения\n",
        "Каждый из этих подуровней имеет остаточную связь вокруг себя, за которой следует нормализация уровня. Результатом каждого подслоя является LayerNorm(x + Sublayer(x)) . Нормализация выполняется по d_model (последняя).\n",
        "\n",
        "В трансформаторе N слоев декодера.\n",
        "\n",
        "Поскольку Q принимает выходные данные от первого блока внимания декодера, а K принимает выходные данные кодировщика, веса внимания представляют важность, придаваемую входу декодера на основе выходных данных кодера. Другими словами, декодер предсказывает следующее слово, глядя на выходные данные кодировщика и самостоятельно присматриваясь к своим собственным выходным данным. См. Демонстрацию выше в разделе «Внимание к скалярному произведению».\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyK4CHznOOhe"
      },
      "source": [
        "The code you provided defines a `DecoderLayer` class in TensorFlow. Let's break it down step by step:\n",
        "\n",
        "1. The `DecoderLayer` class is derived from `tf.keras.layers.Layer`, indicating that it is a custom layer in TensorFlow.\n",
        "\n",
        "2. The `__init__` method is the constructor of the class. It takes four arguments:\n",
        "   - `d_model`: The input and output dimension of the decoder layer, representing the hidden size or embedding dimension.\n",
        "   - `num_heads`: The number of heads in the multi-head attention mechanisms.\n",
        "   - `dff`: The dimension of the hidden layer in the point-wise feed-forward network.\n",
        "   - `rate`: The dropout rate, which is set to 0.1 by default.\n",
        "\n",
        "3. Inside the constructor, the following components are defined:\n",
        "   - `self.mha1` and `self.mha2`: Instances of the `MultiHeadAttention` class, representing the two multi-head attention mechanisms used in the decoder layer. The first one operates on the target sequence, and the second one attends to the encoder output.\n",
        "   - `self.ffn`: An instance of the `point_wise_feed_forward_network` function, representing the point-wise feed-forward network in the decoder layer.\n",
        "   - `self.layernorm1`, `self.layernorm2`, and `self.layernorm3`: Instances of `tf.keras.layers.LayerNormalization`, representing the layer normalization operations applied to the outputs.\n",
        "   - `self.dropout1`, `self.dropout2`, and `self.dropout3`: Instances of `tf.keras.layers.Dropout`, representing dropout layers for regularization.\n",
        "\n",
        "4. The `call` method defines the forward pass of the `DecoderLayer`:\n",
        "   - It takes five arguments: `x` (the input tensor to the decoder layer), `enc_output` (the output from the encoder layer), `training` (a boolean flag indicating whether the model is in training or inference mode), `look_ahead_mask` (a mask tensor to mask future positions in the input), and `padding_mask` (a mask tensor to mask padded positions in the encoder output).\n",
        "   - The first multi-head attention mechanism (`self.mha1`) is applied to the input `x` three times with the same inputs (`x`, `x`, `x`) and the `look_ahead_mask` to obtain `attn1` and the attention weights (`attn_weights_block1`).\n",
        "   - The `attn1` is then passed through the first dropout layer (`self.dropout1`) with the specified dropout rate and the `training` flag.\n",
        "   - The dropout output is added to the original input `x`, and the result is normalized using `self.layernorm1`.\n",
        "   - The second multi-head attention mechanism (`self.mha2`) attends to the encoder output (`enc_output`) and takes the output from the previous step (`out1`) as the query. This is done to capture the relationship between the input and the encoder context.\n",
        "   - The `mha2` output is passed through the second dropout layer (`self.dropout2`) with the specified dropout rate and the `training` flag.\n",
        "   - The dropout output is added to the `out1`, and the result is normalized using `self.layernorm2`.\n",
        "   - The output of the second layer normalization (`out2`) is passed through the point-wise feed-forward network (`self.ffn`) to obtain `ffn_output`.\n",
        "   - The `ffn_output` is then passed through the third dropout layer (`self.dropout3`) with the specified dropout rate and the `training` flag.\n",
        "   - The dropout output is added to the `out2`, and the result is normalized using `self.layernorm3`.\n",
        "\n",
        "\n",
        "   - Finally, the output tensor (`out3`), along with the attention weights from the first and second attention mechanisms, is returned.\n",
        "\n",
        "This `DecoderLayer` class is part of a transformer model and is responsible for processing the decoder inputs and capturing the dependencies between the decoder inputs and the encoder output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9SoX0-vd1hue"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ne2Bqx8k71l0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005f73f7-0371-407a-c2a9-0a78c98fa006"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The `Encoder` consists of:\n",
        "1.   Input Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrJ_GUyyOOhe"
      },
      "source": [
        "Данный код представляет класс `Encoder`, который является частью модели Transformer. Он отвечает за кодирование входной последовательности.\n",
        "\n",
        "В конструкторе класса определены следующие атрибуты:\n",
        "\n",
        "- `num_layers`: количество слоев в кодировщике.\n",
        "- `d_model`: размерность модельного пространства (размерность векторов в кодировщике и декодировщике).\n",
        "- `num_heads`: количество голов в механизме многоголового внимания.\n",
        "- `dff`: размерность скрытого слоя в сети прямого распространения (point-wise feed-forward network).\n",
        "- `input_vocab_size`: размер словаря входных данных.\n",
        "- `maximum_position_encoding`: максимальная длина позиционного кодирования (позиционная эмбеддинг-матрица).\n",
        "- `rate`: коэффициент dropout для регуляризации.\n",
        "\n",
        "В методе `call` выполняется кодирование входной последовательности:\n",
        "\n",
        "- Входные данные `x` проходят через слой эмбеддинга, который преобразует индексы слов в векторы заданной размерности `d_model`.\n",
        "- К векторам эмбеддинга применяется позиционное кодирование (`pos_encoding`), которое добавляет информацию о позиции каждого слова в последовательности.\n",
        "- Векторы позиционного кодирования и эмбеддинга объединяются и проходят через слой dropout для регуляризации.\n",
        "- Затем последовательность проходит через несколько слоев кодировщика (`enc_layers`). Каждый слой кодировщика представлен экземпляром класса `EncoderLayer`.\n",
        "- На выходе получается закодированная последовательность, имеющая размерность `(batch_size, input_seq_len, d_model)`.\n",
        "\n",
        "Таким образом, класс `Encoder` отвечает за преобразование входной последовательности во внутреннее представление модели Transformer, которое будет использоваться в дальнейшем в процессе декодирования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jpEox7gJ8FCI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfvTLLGKOOhf"
      },
      "source": [
        "This code defines a class called `Encoder` that is a part of the Transformer model and is responsible for encoding the input sequence.\n",
        "\n",
        "In the constructor of the class, the following attributes are defined:\n",
        "\n",
        "- `num_layers`: the number of layers in the encoder.\n",
        "- `d_model`: the dimensionality of the model space (the size of the vectors in the encoder and decoder).\n",
        "- `num_heads`: the number of heads in the multi-head attention mechanism.\n",
        "- `dff`: the dimensionality of the hidden layer in the point-wise feed-forward network.\n",
        "- `input_vocab_size`: the size of the input vocabulary.\n",
        "- `maximum_position_encoding`: the maximum length of the positional encoding (position embedding matrix).\n",
        "- `rate`: the dropout rate for regularization.\n",
        "\n",
        "The `call` method performs the encoding of the input sequence:\n",
        "\n",
        "- The length of the input sequence `seq_len` is computed using the `tf.shape` function.\n",
        "- The input data `x` goes through an embedding layer `self.embedding`, which converts word indices into vectors of the specified dimension `d_model`.\n",
        "- The resulting embeddings are then passed through the positional encoding `self.pos_encoding`, which adds positional information to each word in the sequence.\n",
        "- The positional encoding vectors and the embedding vectors are combined and passed through a dropout layer `self.dropout` for regularization.\n",
        "- The sequence then goes through multiple encoder layers `self.enc_layers`. Each encoder layer is an instance of the `EncoderLayer` class. The sequence is passed through each layer in a loop.\n",
        "- The output is the encoded sequence with a shape of `(batch_size, input_seq_len, d_model)`.\n",
        "\n",
        "Thus, the `Encoder` class transforms the input sequence using embeddings and positional encoding, and then applies multiple encoder layers to obtain the encoded representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2S6K77SOOhf"
      },
      "source": [
        "The code snippet demonstrates the usage of the Encoder class to encode an input sequence.\n",
        "\n",
        "First, an instance of the Encoder class is created with the following parameters:\n",
        "\n",
        "num_layers: Number of layers in the encoder, set to 2.\n",
        "d_model: Dimensionality of the model space, set to 512.\n",
        "num_heads: Number of heads in the multi-head attention mechanism, set to 8.\n",
        "dff: Dimensionality of the hidden layer in the point-wise feed-forward network, set to 2048.\n",
        "input_vocab_size: Size of the input vocabulary, set to 8500.\n",
        "maximum_position_encoding: Maximum length of the positional encoding, set to 10000.\n",
        "Then, a random input sequence temp_input of shape (64, 62) is generated using tf.random.uniform. This sequence has a batch size of 64 and a length of 62.\n",
        "\n",
        "The sample_encoder_output is obtained by calling the sample_encoder with the temp_input, training=False, and mask=None arguments. This triggers the call method of the Encoder class.\n",
        "\n",
        "The shape of the sample_encoder_output is printed, which will be (64, 62, 512). This means that the output is a tensor with a batch size of 64, the same sequence length of 62, and a model dimensionality of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8QG9nueFQKXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2340a674-28d8-48f9-b50e-31a78aec2e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 62, 512)\n"
          ]
        }
      ],
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtT7PKzrXkNr"
      },
      "source": [
        " The `Decoder` consists of:\n",
        "1.   Output Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUljAUECOOhf"
      },
      "source": [
        "The code snippet defines the `Decoder` class, which is responsible for decoding the encoded input sequence.\n",
        "\n",
        "The `Decoder` class has the following key components:\n",
        "\n",
        "- In the `__init__` method, the class initializes the model parameters and layers. It takes the following arguments:\n",
        "  - `num_layers`: Number of layers in the decoder.\n",
        "  - `d_model`: Dimensionality of the model space.\n",
        "  - `num_heads`: Number of heads in the multi-head attention mechanism.\n",
        "  - `dff`: Dimensionality of the hidden layer in the point-wise feed-forward network.\n",
        "  - `target_vocab_size`: Size of the target vocabulary.\n",
        "  - `maximum_position_encoding`: Maximum length of the positional encoding.\n",
        "  - `rate`: Dropout rate.\n",
        "\n",
        "- The `call` method is responsible for the forward pass of the decoder. It takes the following arguments:\n",
        "  - `x`: Input to the decoder (target sequence).\n",
        "  - `enc_output`: Output from the encoder.\n",
        "  - `training`: Boolean flag indicating whether the model is in training mode or not.\n",
        "  - `look_ahead_mask`: Mask to prevent the decoder from attending to future positions.\n",
        "  - `padding_mask`: Mask to pad and mask encoder outputs.\n",
        "  \n",
        "- Within the `call` method, the target sequence is processed through the embedding layer and positional encoding. Dropout is applied to the output.\n",
        "\n",
        "- The decoder layers (instances of `DecoderLayer`) are iterated through in a loop. The `call` method of each decoder layer is called with the appropriate arguments.\n",
        "\n",
        "- The attention weights from each decoder layer are stored in a dictionary for visualization and analysis purposes.\n",
        "\n",
        "- The output from the decoder and the attention weights dictionary are returned.\n",
        "\n",
        "Overall, the `Decoder` class in the code snippet is responsible for decoding the input sequence using multiple decoder layers and generating the output sequence along with attention weights at each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "d5_d5-PLQXwY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "        x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "        attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnYDt--zOOhf"
      },
      "source": [
        "The code snippet demonstrates the usage of the `Decoder` class.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "- An instance of the `Decoder` class is created, with the specified parameters: `num_layers=2`, `d_model=512`, `num_heads=8`, `dff=2048`, `target_vocab_size=8000`, and `maximum_position_encoding=5000`.\n",
        "\n",
        "- A random input tensor `temp_input` of shape `(64, 26)` is generated.\n",
        "\n",
        "- The `call` method of the `Decoder` instance is invoked, passing the `temp_input` as the input sequence. Additionally, `enc_output` (the output of the encoder), `training=False`, `look_ahead_mask=None`, and `padding_mask=None` are provided as arguments.\n",
        "\n",
        "- The `call` method returns two values: `output` and `attn`. `output` represents the decoded output sequence of shape `(batch_size, target_seq_len, d_model)`, and `attn` is a dictionary containing attention weights for each decoder layer and attention block.\n",
        "\n",
        "- Finally, the shapes of `output` and `attn['decoder_layer2_block2']` are printed.\n",
        "\n",
        "Therefore, the resulting shapes would be as follows:\n",
        "\n",
        "- `output.shape`: `(batch_size, target_seq_len, d_model)` - It depends on the specific values of `batch_size` and `target_seq_len`, but it would have the same `d_model` value as specified during the creation of the `Decoder` instance (512 in this case).\n",
        "\n",
        "- `attn['decoder_layer2_block2'].shape`: This shape represents the attention weights from the second layer and second attention block of the decoder. The specific shape would depend on the input dimensions and the number of attention heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "a1jXoAMRZyvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff32cc8-4298-4cd2-a708-f8dd0c0541dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input, \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uERO1y54cOKq"
      },
      "source": [
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMUWkqWgOOhf"
      },
      "source": [
        "The code snippet represents the implementation of the `Transformer` model using the Keras `Model` class.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "\n",
        "- The `Transformer` class is defined, inheriting from the `tf.keras.Model` class.\n",
        "\n",
        "- In the constructor (`__init__` method), the model's components are initialized:\n",
        "The constructor (__init__ method) initializes the attributes of the Transformer class. Here's a breakdown of the parameters:\n",
        "\n",
        "num_layers: The number of layers in both the encoder and decoder.\n",
        "d_model: The dimensionality of the model, i.e., the number of hidden units in the encoder and decoder layers.\n",
        "num_heads: The number of attention heads in the multi-head attention mechanism.\n",
        "dff: The dimensionality of the feed-forward network.\n",
        "input_vocab_size: The size of the input vocabulary.\n",
        "target_vocab_size: The size of the target vocabulary.\n",
        "pe_input: The maximum positional encoding for the input sequence.\n",
        "pe_target: The maximum positional encoding for the target sequence.\n",
        "rate: The dropout rate.\n",
        "\n",
        "  - An `Encoder` instance is created, configured with the specified parameters.\n",
        "  - A `Decoder` instance is created, configured with the specified parameters.\n",
        "  - A final dense layer (`tf.keras.layers.Dense`) is added to produce the output logits.\n",
        "\n",
        "- The `call` method is implemented to define the forward pass of the model.\n",
        "  - The `inp` (input) sequence is passed through the encoder, with `enc_padding_mask` to handle padding, producing `enc_output`.\n",
        "  - The `tar` (target) sequence and `enc_output` are passed to the decoder, along with appropriate masks (`look_ahead_mask` and `dec_padding_mask`), resulting in `dec_output` and `attention_weights`.\n",
        "  - The `dec_output` is passed through the final dense layer to obtain the final output logits `final_output`.\n",
        "  - Both `final_output` and `attention_weights` are returned.\n",
        "\n",
        "Therefore, the `Transformer` model takes an input sequence (`inp`) and a target sequence (`tar`) and performs the encoding and decoding steps using the specified number of layers, model dimensions, number of attention heads, and feed-forward dimensions. The output of the model is the final logits for the target sequence, along with the attention weights generated during the decoding process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfOqBNaCOOhg"
      },
      "source": [
        "The `sample_transformer` is an instance of the `Transformer` model with specific configurations:\n",
        "\n",
        "- `num_layers=2`: It has 2 layers in both the encoder and decoder.\n",
        "- `d_model=512`: The model's hidden dimension is set to 512.\n",
        "- `num_heads=8`: The number of attention heads in both the encoder and decoder is 8.\n",
        "- `dff=2048`: The dimension of the feed-forward network in the encoder and decoder is 2048.\n",
        "- `input_vocab_size=8500`: The vocabulary size of the input sequences is 8500.\n",
        "- `target_vocab_size=8000`: The vocabulary size of the target sequences is 8000.\n",
        "- `pe_input=10000`: The maximum position encoding value for the input sequences is set to 10000.\n",
        "- `pe_target=6000`: The maximum position encoding value for the target sequences is set to 6000.\n",
        "\n",
        "To perform a forward pass with the `sample_transformer` model, you pass input sequences (`temp_input`) and target sequences (`temp_target`) to the model's `call` method. Additionally, you can specify other parameters such as `training`, `enc_padding_mask`, `look_ahead_mask`, and `dec_padding_mask` as needed. In this case, `training` is set to `False`, and the masking parameters are set to `None`.\n",
        "\n",
        "The output `fn_out` will have the shape `(batch_size, tar_seq_len, target_vocab_size)`, representing the logits for the target sequences. The `target_vocab_size` dimension contains the scores for each token in the target vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tJ4fbQcIkHW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009d4b55-e5f8-4063-f4ec-dd5dc5c360a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 8000])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n",
        "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lnJn5SLA2ahP"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOmWW--yP3zx"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CrvAQPsOOhg"
      },
      "source": [
        "The provided code defines a custom learning rate schedule in TensorFlow for the Transformer model. The learning rate schedule is implemented as a subclass of `tf.keras.optimizers.schedules.LearningRateSchedule`.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "1. The `CustomSchedule` class is defined, inheriting from `tf.keras.optimizers.schedules.LearningRateSchedule`.\n",
        "2. The constructor method `__init__` initializes the attributes of the schedule. It takes two arguments: `d_model`, which represents the model's dimensionality, and `warmup_steps`, which determines the number of warm-up steps for the learning rate.\n",
        "3. Inside the constructor, the `d_model` attribute is cast to `tf.float32` for numerical consistency.\n",
        "4. The `__call__` method is overridden to compute the learning rate at a given step.\n",
        "5. In the `__call__` method, `arg1` is computed as the reciprocal square root of the step.\n",
        "6. `arg2` is computed as the step multiplied by the reciprocal of the square root of `warmup_steps` raised to the power of `-1.5`.\n",
        "7. The learning rate is calculated by multiplying the reciprocal square root of `d_model` by the minimum of `arg1` and `arg2`.\n",
        "8. The computed learning rate is returned.\n",
        "\n",
        "This custom learning rate schedule implements a warm-up strategy where the learning rate starts low and gradually increases during the warm-up phase (controlled by `warmup_steps`). Afterward, it follows a reciprocal square root decay schedule.\n",
        "\n",
        "This learning rate schedule is commonly used in Transformer models to stabilize training and improve convergence, as it allows the model to initially focus on exploring the solution space with a smaller learning rate and then gradually refine the updates as the learning rate increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iYQdOO1axwEI"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WPVvuMHOOhg"
      },
      "source": [
        "The provided code snippet demonstrates how to use the custom learning rate schedule (`CustomSchedule`) with the Adam optimizer in TensorFlow.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "1. The `learning_rate` variable is instantiated by creating an instance of the `CustomSchedule` class, passing the `d_model` value as an argument. This initializes the learning rate schedule with the given model dimensionality.\n",
        "2. The `optimizer` variable is instantiated as an Adam optimizer by calling `tf.keras.optimizers.Adam()`. It takes several arguments:\n",
        "   - `learning_rate`: The custom learning rate schedule (`CustomSchedule`) object created in the previous step.\n",
        "   - `beta_1`: The exponential decay rate for the first-moment estimates (default is 0.9).\n",
        "   - `beta_2`: The exponential decay rate for the second-moment estimates (default is 0.98).\n",
        "   - `epsilon`: A small constant added to the denominator for numerical stability (default is 1e-9).\n",
        "\n",
        "By using the custom learning rate schedule with the Adam optimizer, the model will automatically adjust its learning rate based on the provided schedule during training. The Adam optimizer is a popular choice for optimizing deep learning models, and the custom learning rate schedule ensures an appropriate learning rate throughout the training process, following the specified formula and adjusting based on the step number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7r4scdulztRx"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GznsjnQKOOhh"
      },
      "source": [
        " plot to visualize how the learning rate varies over a range of train steps using the CustomSchedule class.\n",
        "\n",
        "Here's an explanation of the code:\n",
        "\n",
        "temp_learning_rate_schedule is instantiated as an instance of the CustomSchedule class, using the d_model value as an argument. This initializes the learning rate schedule.\n",
        "plt.plot() is used to create the plot. The x-axis represents the train steps, which are generated using tf.range(40000, dtype=tf.float32), creating a range of 40000 steps.\n",
        "temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)) is called to obtain the learning rate values for each train step. The learning rate schedule is applied to the train steps, resulting in a tensor of learning rate values.\n",
        "The plot is created with the learning rate values on the y-axis and the train steps on the x-axis.\n",
        "plt.ylabel() is used to set the label for the y-axis as \"Learning Rate\".\n",
        "plt.xlabel() is used to set the label for the x-axis as \"Train Step\".\n",
        "The resulting plot illustrates how the learning rate changes as the train steps progress according to the custom learning rate schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "f33ZCgvHpPdG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "2b8f035a-5794-4275-d7a0-1a8c402c4070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxGJtoDuYIHL"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn2JmgleOOhh"
      },
      "source": [
        "The code you provided defines the following functions and metrics for training a Transformer model:\n",
        "\n",
        "1. `loss_object`: This is an instance of the `tf.keras.losses.SparseCategoricalCrossentropy` loss function. It is used to compute the loss between the predicted sequences (`pred`) and the target sequences (`real`). The `from_logits=True` argument indicates that the model's output is not normalized with a softmax activation, and the `reduction='none'` argument specifies that no reduction should be applied to the loss values.\n",
        "\n",
        "2. `loss_function`: This function computes the loss between the predicted sequences (`pred`) and the target sequences (`real`). It applies a mask to ignore padded positions by checking where the `real` values are not equal to 0. The loss is calculated using the `loss_object` and then multiplied by the mask to set the loss values to 0 at padded positions. Finally, the loss values are summed only over non-padded positions and divided by the total number of non-padded positions to get the average loss.\n",
        "\n",
        "3. `accuracy_function`: This function computes the accuracy of the predicted sequences (`pred`) compared to the target sequences (`real`). It first checks where the predicted sequences match the target sequences (`tf.equal(real, tf.argmax(pred, axis=2))`). Then, a mask is applied to ignore padded positions. The function computes the element-wise logical AND between the mask and the accuracies to only consider non-padded positions. The resulting accuracies are cast to `tf.float32`. Finally, the accuracies are summed only over non-padded positions and divided by the total number of non-padded positions to get the average accuracy.\n",
        "\n",
        "4. `train_loss`: This is a `tf.keras.metrics.Mean` metric used to track the average training loss during training.\n",
        "\n",
        "5. `train_accuracy`: This is a `tf.keras.metrics.Mean` metric used to track the average training accuracy during training.\n",
        "\n",
        "These functions and metrics can be used during the training loop of the Transformer model to compute and track the loss and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MlhsJMm0TW_B"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "  \n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "phlyxMnm-Tpx"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UiysUa--4tOU"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQq0gimcOOhh"
      },
      "source": [
        "The `create_masks` function is used to create masks for the Transformer model during training and inference. It takes two arguments: `inp` (input sequence) and `tar` (target sequence). Here's an explanation of each mask it creates:\n",
        "\n",
        "1. **Encoder Padding Mask (`enc_padding_mask`):** This mask is used in the encoder to mask the padding tokens in the input sequence. It is created by calling the `create_padding_mask` function on the `inp` sequence.\n",
        "\n",
        "2. **Combined Mask (`combined_mask`):** This mask is used in the decoder's first attention block. It consists of two sub-masks: the **look-ahead mask** and the **decoder target padding mask**. The look-ahead mask is used to mask the future tokens in the input received by the decoder, preventing it from attending to future positions. The decoder target padding mask is used to mask the padding tokens in the target sequence. The combined mask is created by taking the element-wise maximum of the decoder target padding mask and the look-ahead mask.\n",
        "\n",
        "3. **Decoder Padding Mask (`dec_padding_mask`):** This mask is used in the decoder to mask the padding tokens in the input sequence. It is created by calling the `create_padding_mask` function on the `inp` sequence.\n",
        "\n",
        "The `create_masks` function then returns the `enc_padding_mask`, `combined_mask`, and `dec_padding_mask` as output. These masks are used in the training step of the Transformer model to apply appropriate masking during self-attention and feed-forward network computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ZOJUSB1T8GjM"
      },
      "outputs": [],
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiqpXBOrOOhi"
      },
      "source": [
        "The code snippet provided sets up a checkpointing mechanism for saving and restoring the model during training. Here's a breakdown of the code:\n",
        "\n",
        "1. `checkpoint_path` is the directory path where the checkpoints will be saved.\n",
        "2. `ckpt` is a `tf.train.Checkpoint` object that contains the Transformer model (`transformer`) and the optimizer (`optimizer`).\n",
        "3. `ckpt_manager` is a `tf.train.CheckpointManager` that handles the checkpoint saving and restoring process. It takes the `ckpt` object and the `checkpoint_path` as arguments. The `max_to_keep` parameter specifies the maximum number of checkpoints to keep.\n",
        "4. The code checks if a latest checkpoint exists using `ckpt_manager.latest_checkpoint`. If a checkpoint exists, it restores the model and optimizer states from the latest checkpoint using `ckpt.restore()`.\n",
        "5. Finally, it prints a message indicating that the latest checkpoint has been restored.\n",
        "\n",
        "By using this checkpointing mechanism, you can save the model's parameters and optimizer states during training, allowing you to resume training from the latest checkpoint if training gets interrupted or to load the trained model for inference at a later time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "hNhuYfllndLZ"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Di_Yaa1gf9r"
      },
      "source": [
        "Цель делится на tar_inp и tar_real. tar_inp передается декодеру в качестве входных данных. tar_real - это тот же самый ввод, сдвинутый на 1: в каждом месте tar_input tar_real содержит следующий токен, который должен быть предсказан.\n",
        "\n",
        "Например, sentence = \"SOS Лев в джунглях спит EOS\"\n",
        "\n",
        "tar_inp = \"SOS tar_inp лев в джунглях\"\n",
        "\n",
        "tar_real = \"Лев в джунглях спит EOS\"\n",
        "\n",
        "Преобразователь - это авторегрессивная модель: он делает прогнозы по частям и использует свои выходные данные, чтобы решить, что делать дальше.\n",
        "\n",
        "Во время обучения в этом примере используется принуждение учителя (как в учебнике по созданию текста ). Принуждение учителя передает истинный результат следующему временному шагу независимо от того, что модель предсказывает на текущем временном шаге.\n",
        "\n",
        "Поскольку преобразователь предсказывает каждое слово, самовнимание позволяет ему смотреть на предыдущие слова во входной последовательности, чтобы лучше предсказать следующее слово.\n",
        "\n",
        "Чтобы модель не просматривала ожидаемый результат, в модели используется маска просмотра вперед.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixKPX2NZOOhi"
      },
      "source": [
        "The `train_step` function is defined as a TensorFlow function with a specific input signature. It takes `inp` and `tar` as input tensors of shape `(None, None)` with dtype `tf.int64`.\n",
        "\n",
        "Inside the function, the `tar_inp` is obtained by slicing `tar` to exclude the last token, and `tar_real` is obtained by slicing `tar` to exclude the first token.\n",
        "\n",
        "Next, the masks (`enc_padding_mask`, `combined_mask`, and `dec_padding_mask`) are created using the `create_masks` function.\n",
        "\n",
        "Within a gradient tape context, the `transformer` model is called with `inp`, `tar_inp`, and the masks to obtain `predictions` (the model's output). The loss is calculated using `loss_function` by comparing `tar_real` with `predictions`.\n",
        "\n",
        "Gradients are computed with respect to the trainable variables of the `transformer` model using the tape. Then, the optimizer's `apply_gradients` method is used to update the model's trainable variables based on the gradients.\n",
        "\n",
        "The training loss and accuracy are updated using the `train_loss` and `train_accuracy` metrics, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "LKpoA6q1sJFj"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "iJwmp9OE29oj"
      },
      "outputs": [],
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM2PDWGDJ_8V"
      },
      "source": [
        "Ru is used as the input language and English is the target language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DXI0VrHOOhi"
      },
      "source": [
        "This code snippet represents the training loop for the Transformer model. Here's what it does:\n",
        "\n",
        "- It iterates over the specified number of epochs.\n",
        "- For each epoch, it initializes the training loss and accuracy states.\n",
        "- It iterates over the batches of the training dataset, where each batch contains input (`inp`) and target (`tar`) sequences.\n",
        "- For each batch, it calls the `train_step` function to perform a single training step.\n",
        "- Every 50 batches, it prints the current epoch, batch number, training loss, and training accuracy.\n",
        "- Every 5 epochs, it saves a checkpoint of the model using the `ckpt_manager.save()` method.\n",
        "- After each epoch, it prints the epoch loss and accuracy.\n",
        "- It measures the time taken for each epoch and prints it.\n",
        "- The loop continues until all epochs are completed.\n",
        "\n",
        "This training loop allows the Transformer model to be trained on the provided dataset, and the model's performance is monitored through the loss and accuracy metrics. Checkpoints are saved periodically to allow for model checkpointing and resuming training if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbvmaKNiznHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2ccdd7-ce8a-4f96-d072-107bf8ef2f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 9.0379 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.9769 Accuracy 0.0032\n",
            "Epoch 1 Batch 100 Loss 8.8809 Accuracy 0.0252\n",
            "Epoch 1 Batch 150 Loss 8.7785 Accuracy 0.0327\n",
            "Epoch 1 Batch 200 Loss 8.6515 Accuracy 0.0370\n",
            "Epoch 1 Batch 250 Loss 8.4967 Accuracy 0.0418\n",
            "Epoch 1 Batch 300 Loss 8.3193 Accuracy 0.0504\n",
            "Epoch 1 Batch 350 Loss 8.1334 Accuracy 0.0574\n",
            "Epoch 1 Batch 400 Loss 7.9528 Accuracy 0.0629\n",
            "Epoch 1 Batch 450 Loss 7.7875 Accuracy 0.0680\n",
            "Epoch 1 Batch 500 Loss 7.6423 Accuracy 0.0744\n",
            "Epoch 1 Batch 550 Loss 7.5095 Accuracy 0.0819\n",
            "Epoch 1 Batch 600 Loss 7.3837 Accuracy 0.0898\n",
            "Epoch 1 Batch 650 Loss 7.2616 Accuracy 0.0976\n",
            "Epoch 1 Batch 700 Loss 7.1472 Accuracy 0.1046\n",
            "Epoch 1 Batch 750 Loss 7.0386 Accuracy 0.1116\n",
            "Epoch 1 Batch 800 Loss 6.9353 Accuracy 0.1186\n",
            "Epoch 1 Batch 850 Loss 6.8397 Accuracy 0.1251\n",
            "Epoch 1 Batch 900 Loss 6.7490 Accuracy 0.1313\n",
            "Epoch 1 Batch 950 Loss 6.6654 Accuracy 0.1371\n",
            "Epoch 1 Batch 1000 Loss 6.5853 Accuracy 0.1427\n",
            "Epoch 1 Batch 1050 Loss 6.5112 Accuracy 0.1481\n",
            "Epoch 1 Batch 1100 Loss 6.4406 Accuracy 0.1532\n",
            "Epoch 1 Batch 1150 Loss 6.3749 Accuracy 0.1580\n",
            "Epoch 1 Batch 1200 Loss 6.3132 Accuracy 0.1626\n",
            "Epoch 1 Batch 1250 Loss 6.2546 Accuracy 0.1670\n",
            "Epoch 1 Batch 1300 Loss 6.1991 Accuracy 0.1713\n",
            "Epoch 1 Batch 1350 Loss 6.1475 Accuracy 0.1752\n",
            "Epoch 1 Batch 1400 Loss 6.0973 Accuracy 0.1790\n",
            "Epoch 1 Batch 1450 Loss 6.0507 Accuracy 0.1827\n",
            "Epoch 1 Batch 1500 Loss 6.0055 Accuracy 0.1863\n",
            "Epoch 1 Batch 1550 Loss 5.9626 Accuracy 0.1896\n",
            "Epoch 1 Batch 1600 Loss 5.9214 Accuracy 0.1929\n",
            "Epoch 1 Batch 1650 Loss 5.8828 Accuracy 0.1959\n",
            "Epoch 1 Batch 1700 Loss 5.8451 Accuracy 0.1989\n",
            "Epoch 1 Batch 1750 Loss 5.8101 Accuracy 0.2016\n",
            "Epoch 1 Batch 1800 Loss 5.7761 Accuracy 0.2042\n",
            "Epoch 1 Batch 1850 Loss 5.7428 Accuracy 0.2069\n",
            "Epoch 1 Batch 1900 Loss 5.7107 Accuracy 0.2094\n",
            "Epoch 1 Batch 1950 Loss 5.6804 Accuracy 0.2118\n",
            "Epoch 1 Batch 2000 Loss 5.6501 Accuracy 0.2141\n",
            "Epoch 1 Batch 2050 Loss 5.6212 Accuracy 0.2164\n",
            "Epoch 1 Batch 2100 Loss 5.5931 Accuracy 0.2186\n",
            "Epoch 1 Batch 2150 Loss 5.5659 Accuracy 0.2208\n",
            "Epoch 1 Batch 2200 Loss 5.5402 Accuracy 0.2228\n",
            "Epoch 1 Batch 2250 Loss 5.5148 Accuracy 0.2248\n",
            "Epoch 1 Batch 2300 Loss 5.4902 Accuracy 0.2268\n",
            "Epoch 1 Batch 2350 Loss 5.4654 Accuracy 0.2287\n",
            "Epoch 1 Batch 2400 Loss 5.4416 Accuracy 0.2306\n",
            "Epoch 1 Batch 2450 Loss 5.4183 Accuracy 0.2325\n",
            "Epoch 1 Batch 2500 Loss 5.3960 Accuracy 0.2343\n",
            "Epoch 1 Batch 2550 Loss 5.3744 Accuracy 0.2361\n",
            "Epoch 1 Batch 2600 Loss 5.3529 Accuracy 0.2378\n",
            "Epoch 1 Loss 5.3424 Accuracy 0.2387\n",
            "Time taken for 1 epoch: 353.6798515319824 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.1716 Accuracy 0.3495\n",
            "Epoch 2 Batch 50 Loss 4.1997 Accuracy 0.3316\n",
            "Epoch 2 Batch 100 Loss 4.1989 Accuracy 0.3329\n",
            "Epoch 2 Batch 150 Loss 4.1876 Accuracy 0.3348\n",
            "Epoch 2 Batch 200 Loss 4.1823 Accuracy 0.3345\n",
            "Epoch 2 Batch 250 Loss 4.1721 Accuracy 0.3357\n",
            "Epoch 2 Batch 300 Loss 4.1646 Accuracy 0.3363\n",
            "Epoch 2 Batch 350 Loss 4.1580 Accuracy 0.3369\n",
            "Epoch 2 Batch 400 Loss 4.1483 Accuracy 0.3380\n",
            "Epoch 2 Batch 450 Loss 4.1385 Accuracy 0.3390\n",
            "Epoch 2 Batch 500 Loss 4.1328 Accuracy 0.3394\n",
            "Epoch 2 Batch 550 Loss 4.1225 Accuracy 0.3403\n",
            "Epoch 2 Batch 600 Loss 4.1154 Accuracy 0.3410\n",
            "Epoch 2 Batch 650 Loss 4.1070 Accuracy 0.3420\n",
            "Epoch 2 Batch 700 Loss 4.0971 Accuracy 0.3429\n",
            "Epoch 2 Batch 750 Loss 4.0877 Accuracy 0.3439\n",
            "Epoch 2 Batch 800 Loss 4.0781 Accuracy 0.3447\n",
            "Epoch 2 Batch 850 Loss 4.0705 Accuracy 0.3454\n",
            "Epoch 2 Batch 900 Loss 4.0610 Accuracy 0.3463\n",
            "Epoch 2 Batch 950 Loss 4.0526 Accuracy 0.3470\n",
            "Epoch 2 Batch 1000 Loss 4.0440 Accuracy 0.3480\n",
            "Epoch 2 Batch 1050 Loss 4.0358 Accuracy 0.3488\n",
            "Epoch 2 Batch 1100 Loss 4.0280 Accuracy 0.3496\n",
            "Epoch 2 Batch 1150 Loss 4.0193 Accuracy 0.3505\n",
            "Epoch 2 Batch 1200 Loss 4.0131 Accuracy 0.3512\n",
            "Epoch 2 Batch 1250 Loss 4.0046 Accuracy 0.3521\n",
            "Epoch 2 Batch 1300 Loss 3.9967 Accuracy 0.3529\n",
            "Epoch 2 Batch 1350 Loss 3.9900 Accuracy 0.3536\n",
            "Epoch 2 Batch 1400 Loss 3.9825 Accuracy 0.3545\n",
            "Epoch 2 Batch 1450 Loss 3.9753 Accuracy 0.3553\n",
            "Epoch 2 Batch 1500 Loss 3.9675 Accuracy 0.3562\n",
            "Epoch 2 Batch 1550 Loss 3.9596 Accuracy 0.3570\n",
            "Epoch 2 Batch 1600 Loss 3.9522 Accuracy 0.3577\n",
            "Epoch 2 Batch 1650 Loss 3.9441 Accuracy 0.3586\n",
            "Epoch 2 Batch 1700 Loss 3.9367 Accuracy 0.3594\n",
            "Epoch 2 Batch 1750 Loss 3.9306 Accuracy 0.3601\n",
            "Epoch 2 Batch 1800 Loss 3.9232 Accuracy 0.3609\n",
            "Epoch 2 Batch 1850 Loss 3.9156 Accuracy 0.3617\n",
            "Epoch 2 Batch 1900 Loss 3.9079 Accuracy 0.3626\n",
            "Epoch 2 Batch 1950 Loss 3.9007 Accuracy 0.3634\n",
            "Epoch 2 Batch 2000 Loss 3.8927 Accuracy 0.3642\n",
            "Epoch 2 Batch 2050 Loss 3.8855 Accuracy 0.3650\n",
            "Epoch 2 Batch 2100 Loss 3.8776 Accuracy 0.3659\n",
            "Epoch 2 Batch 2150 Loss 3.8698 Accuracy 0.3668\n",
            "Epoch 2 Batch 2200 Loss 3.8623 Accuracy 0.3676\n",
            "Epoch 2 Batch 2250 Loss 3.8547 Accuracy 0.3684\n",
            "Epoch 2 Batch 2300 Loss 3.8474 Accuracy 0.3693\n",
            "Epoch 2 Batch 2350 Loss 3.8399 Accuracy 0.3701\n",
            "Epoch 2 Batch 2400 Loss 3.8326 Accuracy 0.3708\n",
            "Epoch 2 Batch 2450 Loss 3.8251 Accuracy 0.3716\n",
            "Epoch 2 Batch 2500 Loss 3.8177 Accuracy 0.3724\n",
            "Epoch 2 Batch 2550 Loss 3.8103 Accuracy 0.3732\n",
            "Epoch 2 Batch 2600 Loss 3.8034 Accuracy 0.3739\n",
            "Epoch 2 Loss 3.7997 Accuracy 0.3743\n",
            "Time taken for 1 epoch: 147.70274233818054 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.3807 Accuracy 0.4230\n",
            "Epoch 3 Batch 50 Loss 3.3841 Accuracy 0.4181\n",
            "Epoch 3 Batch 100 Loss 3.3700 Accuracy 0.4193\n",
            "Epoch 3 Batch 150 Loss 3.3723 Accuracy 0.4194\n",
            "Epoch 3 Batch 200 Loss 3.3740 Accuracy 0.4196\n",
            "Epoch 3 Batch 250 Loss 3.3673 Accuracy 0.4206\n",
            "Epoch 3 Batch 300 Loss 3.3613 Accuracy 0.4212\n",
            "Epoch 3 Batch 350 Loss 3.3546 Accuracy 0.4219\n",
            "Epoch 3 Batch 400 Loss 3.3487 Accuracy 0.4223\n",
            "Epoch 3 Batch 450 Loss 3.3429 Accuracy 0.4231\n",
            "Epoch 3 Batch 500 Loss 3.3384 Accuracy 0.4236\n",
            "Epoch 3 Batch 550 Loss 3.3295 Accuracy 0.4244\n",
            "Epoch 3 Batch 600 Loss 3.3257 Accuracy 0.4248\n",
            "Epoch 3 Batch 650 Loss 3.3224 Accuracy 0.4252\n",
            "Epoch 3 Batch 700 Loss 3.3165 Accuracy 0.4259\n",
            "Epoch 3 Batch 750 Loss 3.3098 Accuracy 0.4266\n",
            "Epoch 3 Batch 800 Loss 3.3020 Accuracy 0.4274\n",
            "Epoch 3 Batch 850 Loss 3.2975 Accuracy 0.4279\n",
            "Epoch 3 Batch 900 Loss 3.2930 Accuracy 0.4284\n",
            "Epoch 3 Batch 950 Loss 3.2872 Accuracy 0.4291\n",
            "Epoch 3 Batch 1000 Loss 3.2821 Accuracy 0.4298\n",
            "Epoch 3 Batch 1050 Loss 3.2764 Accuracy 0.4304\n",
            "Epoch 3 Batch 1100 Loss 3.2708 Accuracy 0.4312\n",
            "Epoch 3 Batch 1150 Loss 3.2651 Accuracy 0.4319\n",
            "Epoch 3 Batch 1200 Loss 3.2592 Accuracy 0.4326\n",
            "Epoch 3 Batch 1250 Loss 3.2536 Accuracy 0.4332\n",
            "Epoch 3 Batch 1300 Loss 3.2483 Accuracy 0.4337\n",
            "Epoch 3 Batch 1350 Loss 3.2434 Accuracy 0.4342\n",
            "Epoch 3 Batch 1400 Loss 3.2375 Accuracy 0.4348\n",
            "Epoch 3 Batch 1450 Loss 3.2326 Accuracy 0.4354\n",
            "Epoch 3 Batch 1500 Loss 3.2272 Accuracy 0.4361\n",
            "Epoch 3 Batch 1550 Loss 3.2228 Accuracy 0.4365\n",
            "Epoch 3 Batch 1600 Loss 3.2179 Accuracy 0.4371\n",
            "Epoch 3 Batch 1650 Loss 3.2130 Accuracy 0.4377\n",
            "Epoch 3 Batch 1700 Loss 3.2086 Accuracy 0.4382\n",
            "Epoch 3 Batch 1750 Loss 3.2036 Accuracy 0.4388\n",
            "Epoch 3 Batch 1800 Loss 3.1994 Accuracy 0.4393\n",
            "Epoch 3 Batch 1850 Loss 3.1953 Accuracy 0.4397\n",
            "Epoch 3 Batch 1900 Loss 3.1909 Accuracy 0.4402\n",
            "Epoch 3 Batch 1950 Loss 3.1862 Accuracy 0.4407\n",
            "Epoch 3 Batch 2000 Loss 3.1816 Accuracy 0.4412\n",
            "Epoch 3 Batch 2050 Loss 3.1766 Accuracy 0.4417\n",
            "Epoch 3 Batch 2100 Loss 3.1722 Accuracy 0.4423\n",
            "Epoch 3 Batch 2150 Loss 3.1679 Accuracy 0.4428\n",
            "Epoch 3 Batch 2200 Loss 3.1638 Accuracy 0.4432\n",
            "Epoch 3 Batch 2250 Loss 3.1601 Accuracy 0.4435\n",
            "Epoch 3 Batch 2300 Loss 3.1564 Accuracy 0.4439\n",
            "Epoch 3 Batch 2350 Loss 3.1520 Accuracy 0.4445\n",
            "Epoch 3 Batch 2400 Loss 3.1480 Accuracy 0.4449\n",
            "Epoch 3 Batch 2450 Loss 3.1444 Accuracy 0.4453\n",
            "Epoch 3 Batch 2500 Loss 3.1404 Accuracy 0.4459\n",
            "Epoch 3 Batch 2550 Loss 3.1361 Accuracy 0.4464\n",
            "Epoch 3 Batch 2600 Loss 3.1327 Accuracy 0.4467\n",
            "Epoch 3 Loss 3.1311 Accuracy 0.4468\n",
            "Time taken for 1 epoch: 145.07211112976074 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.1233 Accuracy 0.4522\n",
            "Epoch 4 Batch 50 Loss 2.9128 Accuracy 0.4721\n",
            "Epoch 4 Batch 100 Loss 2.9020 Accuracy 0.4732\n",
            "Epoch 4 Batch 150 Loss 2.8854 Accuracy 0.4753\n",
            "Epoch 4 Batch 200 Loss 2.8803 Accuracy 0.4756\n",
            "Epoch 4 Batch 250 Loss 2.8707 Accuracy 0.4770\n",
            "Epoch 4 Batch 300 Loss 2.8723 Accuracy 0.4763\n",
            "Epoch 4 Batch 350 Loss 2.8767 Accuracy 0.4757\n",
            "Epoch 4 Batch 400 Loss 2.8824 Accuracy 0.4744\n",
            "Epoch 4 Batch 450 Loss 2.8818 Accuracy 0.4744\n",
            "Epoch 4 Batch 500 Loss 2.8788 Accuracy 0.4746\n",
            "Epoch 4 Batch 550 Loss 2.8775 Accuracy 0.4746\n",
            "Epoch 4 Batch 600 Loss 2.8758 Accuracy 0.4749\n",
            "Epoch 4 Batch 650 Loss 2.8721 Accuracy 0.4756\n",
            "Epoch 4 Batch 700 Loss 2.8695 Accuracy 0.4759\n",
            "Epoch 4 Batch 750 Loss 2.8685 Accuracy 0.4760\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6APsFrgImLW"
      },
      "source": [
        "Для оценки используются следующие шаги:\n",
        "\n",
        "Закодируйте входное предложение с помощью русского токенизатора ( tokenizer_pt ). Кроме того, добавьте начальный и конечный токены, чтобы ввод был эквивалентен тому, с чем обучается модель. Это вход энкодера.\n",
        "Вход декодера - это start token == tokenizer_en.vocab_size .\n",
        "Рассчитайте маски заполнения и маски прогнозирования.\n",
        "Затем decoder выводит прогнозы, глядя на encoder output и собственные выходные данные (самовнимание).\n",
        "Выберите последнее слово и вычислите его argmax.\n",
        "Конкатентируйте предсказанное слово на вход декодера при передаче его в декодер.\n",
        "В этом подходе декодер предсказывает следующее слово на основе предсказанных им предыдущих слов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4YWWSNjOOhi"
      },
      "source": [
        "The `evaluate` function takes an input sentence in Portuguese and generates its corresponding translation in English using the trained Transformer model. Here's how it works:\n",
        "\n",
        "- The input sentence is tokenized using the Portuguese tokenizer (`tokenizer_pt`) and appended with start and end tokens.\n",
        "- The encoder input is created by expanding the dimensions of the tokenized input sentence.\n",
        "- The decoder input starts with the English start token (`tokenizer_en.vocab_size`).\n",
        "- The `output` tensor represents the decoder input sequence, which is expanded with an extra dimension.\n",
        "- In a loop that runs for a maximum length (`MAX_LENGTH`) or until the end token is predicted:\n",
        "  - The padding masks (`enc_padding_mask`, `combined_mask`, `dec_padding_mask`) are created using the encoder input and the current decoder output.\n",
        "  - The Transformer model is called with the encoder input, current decoder output, and the created masks to generate predictions and attention weights.\n",
        "  - The predicted next token is selected by taking the token with the highest probability (`argmax`) from the predictions.\n",
        "  - If the predicted token is the end token (`tokenizer_en.vocab_size + 1`), the generated output sequence is returned along with the attention weights.\n",
        "  - Otherwise, the predicted token is concatenated with the output sequence and the loop continues.\n",
        "- Finally, the generated output sequence is returned (after removing the extra dimension) along with the attention weights.\n",
        "\n",
        "This function allows you to provide an input sentence and obtain its translation using the trained Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5buvMlnvyrFm"
      },
      "outputs": [],
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_pt.vocab_size]\n",
        "  end_token = [tokenizer_pt.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input = [tokenizer_en.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_en.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYRvIBhoOOhi"
      },
      "source": [
        "The `plot_attention_weights` function is used to visualize the attention weights for each head of a specific layer in the Transformer model. Here's how it works:\n",
        "\n",
        "- The function takes the attention tensor, input sentence, generated result, and the layer index as inputs.\n",
        "- It creates a figure with a size of (16, 8) to accommodate the subplots for each attention head.\n",
        "- The input sentence is tokenized using the Portuguese tokenizer (`tokenizer_pt`).\n",
        "- The attention tensor is squeezed to remove the extra dimensions, leaving the attention weights with the shape (num_heads, target_seq_len, input_seq_len+2).\n",
        "- For each head in the attention tensor, a subplot is added to the figure.\n",
        "- The attention weights for the current head are visualized using a color map (`cmap='viridis'`).\n",
        "- The x-axis tick labels represent the tokens in the input sentence (start token, Portuguese tokens, and end token), decoded using the Portuguese tokenizer.\n",
        "- The y-axis tick labels represent the tokens in the generated result, decoded using the English tokenizer (`tokenizer_en`).\n",
        "- The x-axis label of each subplot shows the corresponding head index.\n",
        "- The plot is displayed with tight layout to ensure proper spacing between subplots.\n",
        "\n",
        "This function allows you to visualize the attention weights for each head of a specific layer, providing insights into the attention mechanism of the Transformer model during translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "CN-BV43FMBej"
      },
      "outputs": [],
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = tokenizer_pt.encode(sentence)\n",
        "  \n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence)+2))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "    \n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "        fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                        if i < tokenizer_en.vocab_size], \n",
        "                       fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTnUGr6TOOhj"
      },
      "source": [
        "The `translate` function is used to translate a given input sentence from Portuguese to English using the trained Transformer model. Here's how it works:\n",
        "\n",
        "- The function takes a sentence as input and calls the `evaluate` function to generate the translation result and attention weights.\n",
        "- The translation result is decoded using the English tokenizer (`tokenizer_en`) to obtain the predicted sentence.\n",
        "- The input sentence and the predicted translation are printed.\n",
        "- If the `plot` parameter is provided (not empty), the `plot_attention_weights` function is called to visualize the attention weights. The `plot` parameter specifies the layer for which the attention weights should be plotted.\n",
        "- The attention weights are used to visualize the alignment between the input sentence and the generated translation at the specified layer.\n",
        "\n",
        "This function allows you to translate a sentence and optionally visualize the attention weights to gain insights into the translation process of the Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "lU2_yG_vBGza"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  \n",
        "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
        "                                            if i < tokenizer_en.vocab_size])  \n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BpsHP-x3Lnq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415e89bf-6fb0-4d8c-df00-aac2eb689029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Осенним вечером шёл дождь!\n",
            "Predicted translation: the slot was going to go to the rain evening .\n"
          ]
        }
      ],
      "source": [
        "translate('Осенним вечером шёл дождь!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "gmLTN9cgL3ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415cac3f-e9cd-41c6-8d06-cc4e75121c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: он собирается домой\n",
            "Predicted translation: he is going to go home .\n"
          ]
        }
      ],
      "source": [
        "translate('он собирается домой')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHzndZ1xNHKn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s_qNSzzyaCbD"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}